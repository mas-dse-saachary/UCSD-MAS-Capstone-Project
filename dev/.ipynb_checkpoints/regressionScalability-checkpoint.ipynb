{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to automate regression analysis techniques in order to create a system that selects the best model iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir, file_name):\n",
    "    return pd.read_csv(data_dir + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_initial_feature(df):\n",
    "    #create bivariate models using best R2\n",
    "    #select best feature based on highest univar R2\n",
    "    return initial_feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_all_features(df, initial_feature_name, method):\n",
    "    #create feature ranking list with one of multiple methods e.g. Shapiro, Lasso, Ridge, etc.\n",
    "    #force the initial_feature_name into position 1 in list\n",
    "    return ranked_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_next_best_feature(df, ranked_features_list, number_of_features):\n",
    "    #create n models with initial feature and n next best features based on parameter 'number of features'\n",
    "    #select the best model in terms of RMSE\n",
    "    #return the features used by best model, and model type\n",
    "    return best_features, model_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Building Simple Scalable Solution with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"Regression Scalability\")\\\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying data transformations from regressionDiagnostics nb - these might end up changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = pd.read_csv('listings_augmented_2018-05-06_V1.csv',low_memory=False)\n",
    "\n",
    "listings = listings.drop(columns=['listing_id','id','scrape_id','host_id',\n",
    "                                  'zipcode','has_availability','license',\n",
    "                                  'Unnamed: 0', 'thumbnail_url', 'medium_url',\n",
    "                                  'picture_url', 'xl_picture_url', 'host_url',\n",
    "                                  'host_name', 'host_thumbnail_url', 'host_picture_url',\n",
    "                                  'price_x', 'amenities', 'amenities_set', 'bed_type'\n",
    "                                 ])\n",
    "\n",
    "y = listings['price_y'].fillna(listings['price_y'].mean())\n",
    "\n",
    "X = listings.copy()\n",
    "#NB: not dropping the price in this nb because it is needed by Spark's ML methods\n",
    "#X = X.drop(columns='price_y')\n",
    "\n",
    "X_num = X.select_dtypes(include=['int64','float64'])\n",
    "X_num = X_num.replace([np.inf, -np.inf], np.nan)\n",
    "X_num = X_num.fillna(X_num.mean())\n",
    "X_1 = X_num[X_num.columns.drop(X_num[list(X_num.filter(regex='Topic'))])]\n",
    "data_scaled = pd.DataFrame(preprocessing.scale(X_1),columns = X_1.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "colData = sc.parallelize(data_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rSubset(row):\n",
    "     \n",
    "    # return list of all subsets of length r\n",
    "    # to deal with duplicate subsets use \n",
    "    # set(list(combinations(arr, r)))\n",
    "    combos = []\n",
    "    for i in range(len(data_scaled)-1):\n",
    "        combos.append(list(combinations(row, i)))\n",
    "    return combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-df2a97bac4f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcolData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrSubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/pauldefusco/Documents/spark-2.3.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/Documents/spark-2.3.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/Documents/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1160\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/Users/pauldefusco/Documents/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/Documents/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "colData.map(rSubset).map(lambda x: x).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF = spark.createDataFrame(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'parallelize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e1a44cf6f7d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'parallelize'"
     ]
    }
   ],
   "source": [
    "spark.parallelize(data_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Had loaded spark df from file - but prefer to do data manipulation in pandas and load ultimate df\n",
    "#df = spark.read.format('csv').options(header='true', inferSchema='true').load(\"listings_augmented_2018-05-06_V1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.3.0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=10, regParam=0, elasticNetParam=0, fitIntercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=sparkDF.rdd.map(lambda x:(Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr.setFitIntercept(True)\n",
    "lrModel = lr.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,-0.01025100144913379,-0.01025100144913379,0.0,-0.036342098352685176,0.05188618482296399,-0.041180604772694536,-0.0015449116526528792,0.029802264266534984,-0.01911272062653588,0.00794163501707124,0.0,0.0,0.0,0.0,0.0525472100196558,0.0,-0.013721210548706482,0.007090436182805309,0.04592967563966534,0.024952434634345842,-0.004879163416283691,-0.036614368260247596,-0.03244949374408551,0.006443624308387956,0.01622069747733493,0.03566345899732449,0.017897773134429074,0.0013042626976296112,-0.11413696688629542,0.042237831855784655,-0.08291772875319363,-0.005450675117609569,-0.09905914936818977,-0.008361658460004378,0.0050824410594768644,-0.0028965147280422252,-0.017114198746592695,-0.029193162928453028,-0.0029927522573441026,-0.022765772108482102,0.044546769802219416,0.019675542796952557,-0.018808668348632877,-0.013913001062699482,0.02401102789920411,-0.011755218148775705,0.004304520217713648,-0.01192522531513526,-0.016324236167148624,0.011019274243812183,0.013719760298903831,-0.0035922848311242123,0.036141572633334444,-0.0151248951557212,-0.00870264199544652,-0.015536346144969094,-0.007454990211560847,-0.028798523304105716,-0.0075474230065524514,0.01495609082448788,0.019673146250078332,-0.014055527346124298,-0.0012540416504781332,0.03916899701075492,0.0005447086646652395,-0.011148741284450247,-0.02648500592453069,0.019934849938915634,-0.0022620635662031433,0.0069584843622357544,-0.008902982445450165,-0.020950876413439692,-0.0022356079019730745,0.0022923564510553157,-0.01626270994145121,-0.006485437899757924,-0.023096249854978976,1.2371488766863199e-05,-0.020278510149496526,0.03486949937317797,-0.00601034836925342,0.007530368147976845,-0.009700759851168344,-0.03489500855528286,0.004624915455721599,0.003758445965475554,0.028280623611924155,0.014485671577298208,-0.003842043404413844,0.0029978157794541553,-0.005197966883221302,-0.020662969634412737,-0.00820985228979197,0.008263272113201839,-0.019424064459849004,-0.02312005888304298,0.0010451758893608933,-0.007689988566453147,-0.015950496260725106,-0.013523140317694733,-0.004476423711498645,0.009178622171989602,0.005586167856011512,-0.02335624621687644,-0.018985202147074567,-0.005748821070573229,-0.02645444271906826,-0.018904699189657752,-0.010907496372681695,-0.000987866200626235,0.003758968548092239,0.029671136250050013,0.0197111065329337,0.01291827338441128,0.0007139831225654465,0.0020137401697202743,-0.0039067759330558135,-0.0001003950610329567,0.020204632406591472,0.0026993570097177265,0.012456820915550627,-0.0056607344901882005,-0.007229402120128375,-0.003844455882019991,-0.01734135908967013,-0.0006879420417410429,0.006495431840171846,0.0069368753477069625,-0.016427284153500425,-0.006500384733647724,0.0006578533053350004,-0.015005389544031756,-0.013034460981802519,-0.0012020046931740876,0.013191263528081946,-0.00512174409317046,0.019587082665737145,0.012524794508129761,-0.0262689610791179,-0.01267054714017428,0.007276326917724244,0.006354695633346774,-0.011072793148628904,0.0023811478473236844,-0.0021055824254382627,0.018271859454825925,-0.016174187424999067,-0.009304249730624273,-0.0005546077153277811,-0.010546829912461005,-0.03170945303831664,-0.026615103718804392,0.004143753964260765,-0.006983638078312916,0.011934669350292979,-0.007455959809239063,-0.014464980205570728,-0.0025717480372936773,0.010547441629698135,0.006049876441416514,0.020188021634391713,0.026658235126467367,0.008432728919278978,-0.006292769658422223,0.021095108472916353,0.0052885959401108825,-0.0009326007949769834,0.002206337383565081,-0.021294812294679964,-0.01681563301891552,0.02685604371185283,0.013377517239421023,0.01137317722432394,-0.0050736005287662055,-0.020091268000611784,0.004393236146094909,-0.009706886531512398,-0.021369773816051322,-0.02240899333102751,-0.005247995165035895,-0.001570027808333296,-0.002394811794113352,-0.003048031725565037,0.0007161133545802059,0.016139982356748114,0.027156419082944937,0.024445176270696218,0.0527775461380299,0.004800230985317786,-0.0034346723037593572,-0.007070521270046713,0.00274713377351982,-0.0058351322583352835,-0.00018639136190152115,0.0037609441054255966,0.001343144208999508,-0.008961925783239087,-0.006682815885991915,0.030377920137431326,0.0183584141332921,0.020220927489804866,0.025762628805476025,0.002463920603401656,-0.021956273798354305,0.008114407914688296,0.0006682199088702637,0.0011148970414257616,-0.0025305851488178956,-0.006952985874942658,-0.003288768156578896,0.0011071977343403365,0.004261088280601888,-0.01020386515861001,-0.008752175492918826,-0.004044743476240854,0.006826513552880057,0.0015353279848498412,0.002365636003210425,-0.0010508252623779753,0.0016561281904249768,0.001462512208271177,0.02415967639680677,0.005304681951277133,0.010598756072187576,-0.011264329801351074,-0.03055098580962972,-0.012157772627539632,0.012280799262734293,0.0004623916203806182,-0.0062847149153469,0.0038822500351772034,0.023822857612833283,0.003084346501200411,0.006664034590367902,0.006028558771852156,0.0014463452988918728,0.0032224796810303665,-0.00614050736817546,0.005930565653032511,-0.010854843141847882,0.003139376742270703,-0.004442922325501941,0.007125507935466229,0.00816361832437436,-0.005820559953692222,-0.013327381123385395,-0.02331108133336043,-0.0074512690412959805,0.02421115899552576,0.03504756023831248,0.008794961864666238,0.022814284525730182,0.000832698327141451,-0.010882396980764068,0.003714391882916467,0.005453176755453718,-0.0009682267399174229,0.010174313502961203,0.00956372276374124,0.005131875406233765,-0.01604853737543796,0.007754474306464675,-0.01167789663416053,-0.012968137147661258,0.09345508414948923,0.23190627126101684,0.05892757875564565,0.00018218303299085344,-0.03658103824433574,-0.0422831985611371,0.005399404035883467,0.015730212404611563,0.04924806254092772,0.01635215807667189,-0.004598504965811835,-0.027641017323703266,-0.05447946326283749,0.01609469522361965,0.011807687633623514,-0.012390431003909892,0.020920781714714023,-0.007374298606965227,0.015115976516959532,-0.018864600142317046,0.04081188059358578,-0.0020369206190353362,-0.01323565075979641,0.01054343632653187,0.18904314560689717,0.0290605517065233,0.03332061159008424,-0.02858501174621694,0.007038139835773311,-0.010230415006885011,-0.011894349108252537,-0.01056453975596806,-0.03366204333717416,-0.003755027051888135,0.03308298627065423,0.025720627031408275,0.0183729872388809,0.023490846993967633,-0.0030385274999885827,0.033324195250303106,-0.004379361904935275,-0.0253355583085705,-0.024112906970016314,0.014999448056081235,0.006814379461186536,-0.06883715015548732,0.06803689819725582,0.06439537031385316,-0.01785854527986749,0.018573858170162164,-0.010042621831094566]\n",
      "Intercept: -1.00379927567e-14\n",
      "numIterations: 11\n",
      "objectiveHistory: [0.5000000000000001, 0.4241515575041747, 0.41499952691876346, 0.4070013524342572, 0.3826523092574843, 0.3730051302652172, 0.3617684867570493, 0.35981364087099854, 0.3577619042376177, 0.3564519109830645, 0.3546197970019396]\n",
      "+------------------+\n",
      "|         residuals|\n",
      "+------------------+\n",
      "|3.4726822174515526|\n",
      "|3.0152367622271443|\n",
      "| 3.891176999895735|\n",
      "| 2.965070462392914|\n",
      "|3.7282255186993063|\n",
      "|  3.62099207744691|\n",
      "|3.4072812674916437|\n",
      "|2.9321353073764893|\n",
      "| 4.156834048794156|\n",
      "| 3.204544892410344|\n",
      "|3.3372119344543663|\n",
      "|1.9318901644144146|\n",
      "|1.7683499392903514|\n",
      "|1.3696567248283058|\n",
      "|2.5081167695639834|\n",
      "|  1.46567020210938|\n",
      "|3.1568070759943687|\n",
      "|3.2660674560770677|\n",
      "|2.3484066278969733|\n",
      "| 2.191903834767799|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 0.842164\n",
      "r2: 0.290760\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(data_scaled)-1\n",
    "for i in range(n)\n",
    "    data=sparkDF.rdd.map(lambda x:(Vectors.dense(x[0:n]), x[-1])).toDF([\"features\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst = ['col1', 'col2', 'col3', 'col4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rSubset(arr, r):\n",
    " \n",
    "    # return list of all subsets of length r\n",
    "    # to deal with duplicate subsets use \n",
    "    # set(list(combinations(arr, r)))\n",
    "    return list(combinations(arr, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_scaled)-1):\n",
    "    print rSubset(data_scaled.columns, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "a = sc.parallelize([\n",
    "    (1, [1,2,3,4]),\n",
    "    (2, [3,4,5,6]),\n",
    "    (3, [-1,2,3,4])\n",
    "  ])\n",
    "\n",
    "def combinations(row):\n",
    "  l = row[1]\n",
    "  k = row[0]\n",
    "  return [(k, v) for v in itertools.combinations(l, 2)]\n",
    "\n",
    "a.map(combinations).flatMap(lambda x: x).take(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
