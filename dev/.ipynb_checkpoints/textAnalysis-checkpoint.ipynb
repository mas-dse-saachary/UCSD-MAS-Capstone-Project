{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to Explore Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used was created in the regressions notebook under the section dated 4/10/18 i.e. it is three dataframes of listings with average price for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting with listing descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings = pd.read_csv('Datasources/inside_airbnb/listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'listing_url', u'scrape_id', u'last_scraped', u'name',\n",
       "       u'summary', u'space', u'description', u'experiences_offered',\n",
       "       u'neighborhood_overview', u'notes', u'transit', u'access',\n",
       "       u'interaction', u'house_rules', u'thumbnail_url', u'medium_url',\n",
       "       u'picture_url', u'xl_picture_url', u'host_id', u'host_url',\n",
       "       u'host_name', u'host_since', u'host_location', u'host_about',\n",
       "       u'host_response_time', u'host_response_rate', u'host_acceptance_rate',\n",
       "       u'host_is_superhost', u'host_thumbnail_url', u'host_picture_url',\n",
       "       u'host_neighbourhood', u'host_listings_count',\n",
       "       u'host_total_listings_count', u'host_verifications',\n",
       "       u'host_has_profile_pic', u'host_identity_verified', u'street',\n",
       "       u'neighbourhood', u'neighbourhood_cleansed',\n",
       "       u'neighbourhood_group_cleansed', u'city', u'state', u'zipcode',\n",
       "       u'market', u'smart_location', u'country_code', u'country', u'latitude',\n",
       "       u'longitude', u'is_location_exact', u'property_type', u'room_type',\n",
       "       u'accommodates', u'bathrooms', u'bedrooms', u'beds', u'bed_type',\n",
       "       u'amenities', u'square_feet', u'price', u'weekly_price',\n",
       "       u'monthly_price', u'security_deposit', u'cleaning_fee',\n",
       "       u'guests_included', u'extra_people', u'minimum_nights',\n",
       "       u'maximum_nights', u'calendar_updated', u'has_availability',\n",
       "       u'availability_30', u'availability_60', u'availability_90',\n",
       "       u'availability_365', u'calendar_last_scraped', u'number_of_reviews',\n",
       "       u'first_review', u'last_review', u'review_scores_rating',\n",
       "       u'review_scores_accuracy', u'review_scores_cleanliness',\n",
       "       u'review_scores_checkin', u'review_scores_communication',\n",
       "       u'review_scores_location', u'review_scores_value', u'requires_license',\n",
       "       u'license', u'jurisdiction_names', u'instant_bookable',\n",
       "       u'cancellation_policy', u'require_guest_profile_picture',\n",
       "       u'require_guest_phone_verification', u'calculated_host_listings_count',\n",
       "       u'reviews_per_month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Text columns:\n",
    "space', u'description', u'experiences_offered',\n",
    "       u'neighborhood_overview', u'notes', u'transit', u'access',\n",
    "       u'interaction', u'house_rules'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some basic data exploration to see what can be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_features = ['space', 'description', 'experiences_offered', 'neighborhood_overview', 'notes', 'transit', 'access', 'interaction', 'house_rules']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6608"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name: space, Nulls Count: 1635,\n",
      "Column Name: description, Nulls Count: 2,\n",
      "Column Name: experiences_offered, Nulls Count: 0,\n",
      "Column Name: neighborhood_overview, Nulls Count: 2471,\n",
      "Column Name: notes, Nulls Count: 3375,\n",
      "Column Name: transit, Nulls Count: 2636,\n",
      "Column Name: access, Nulls Count: 2273,\n",
      "Column Name: interaction, Nulls Count: 2530,\n",
      "Column Name: house_rules, Nulls Count: 1642,\n"
     ]
    }
   ],
   "source": [
    "#How many rows are null\n",
    "for i in text_features:\n",
    "    print(\"Column Name: %s, Nulls Count: %i,\" %(i, listings[i].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Aquatica Waterpark, Sleep train Amphitheater, ...\n",
       "1    Your spacious room awaiting is with a Queen Si...\n",
       "2    This is an immaculate 3 bedroom, 2 1/2 bath co...\n",
       "3    This 2 Story TownHome  is close to Otay Ranch ...\n",
       "4    Hello; we are offering a private secluded bedr...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings['description'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(doc):\n",
    "    features = defaultdict(int)\n",
    "\n",
    "    for token in tokenize(doc):\n",
    "        features[token] += 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = map(vectorize, listings['description'].iloc[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(listings['description'].iloc[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x42 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 42 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf  = TfidfVectorizer()\n",
    "corpus = tfidf.fit_transform(listings['description'].iloc[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import unicodedata\\nfrom sklearn.base import BaseEstimator, TransformerMixin\\n\\n\\nclass TextNormalizer(BaseEstimator, TransformerMixin):\\n\\n    def __init__(self, language='english'):\\n        self.stopwords  = set(nltk.corpus.stopwords.words(language))\\n        self.lemmatizer = WordNetLemmatizer()\\n\\n    def is_punct(self, token):\\n        return all(\\n            unicodedata.category(char).startswith('P') for char in token\\n        )\\n\\n    def is_stopword(self, token):\\n        return token.lower() in self.stopwords\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import unicodedata\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "            #('norm', TextNormalizer()),\n",
    "            ('tfidf', CountVectorizer(\n",
    "                                      preprocessor=None, lowercase=False)),\n",
    "            ('model', LatentDirichletAllocation(n_components=3)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.98735993,  0.00632963,  0.00631044]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_transform(listings['description'].iloc[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.26902133,  2.05562914,  2.06017888,  2.83470762,  2.03250305,\n",
       "         1.23443982,  1.24304619,  1.29663275,  1.23884115,  1.26656166,\n",
       "         1.26127882,  1.23075161,  1.25366604,  1.23870447,  1.28216689,\n",
       "         2.04659531,  1.26188375,  1.23180368,  1.2533177 ,  1.2524989 ,\n",
       "         2.01542106,  1.24265827,  1.25794635,  2.01295537,  1.23331509,\n",
       "         1.26873006,  1.23358847,  2.03035774,  1.22327871,  1.25909251,\n",
       "         1.25130389,  2.01172405,  1.23321328,  1.23142629,  2.05194809,\n",
       "         1.2819665 ,  1.25196099,  1.28441775,  1.26944471,  1.25952438,\n",
       "         1.27542894,  2.06231565,  1.24849447],\n",
       "       [ 0.47102121,  0.50494043,  0.44909994,  0.50102125,  0.49621477,\n",
       "         0.46451952,  0.46609582,  0.4728808 ,  0.46213228,  0.47303494,\n",
       "         0.45750403,  0.46142482,  0.46763878,  0.48492776,  0.50647068,\n",
       "         0.4890672 ,  0.47499255,  0.47723054,  0.46807074,  0.49891561,\n",
       "         0.51459397,  0.49085577,  0.53188074,  0.51154629,  0.4116325 ,\n",
       "         0.45180079,  0.47219973,  0.53029275,  0.48028172,  0.47781833,\n",
       "         0.44630933,  0.48908862,  0.50122095,  0.5433571 ,  0.44973176,\n",
       "         0.50051557,  0.49415517,  0.48476088,  0.45278504,  0.48377921,\n",
       "         0.46709978,  0.4939156 ,  0.48855461],\n",
       "       [ 0.43550389,  0.43388656,  0.48868111,  0.50499623,  0.46603991,\n",
       "         0.4849572 ,  0.47601014,  0.4869103 ,  0.47894585,  0.45106865,\n",
       "         0.43467467,  0.48528201,  0.49532063,  0.45093736,  0.48492704,\n",
       "         0.47464857,  0.46353329,  0.46094558,  0.49172128,  0.4339783 ,\n",
       "         0.46400656,  0.44877675,  0.45896762,  0.50228534,  0.50653913,\n",
       "         0.45058572,  0.46675716,  0.46475081,  0.46675159,  0.4568568 ,\n",
       "         0.48843178,  0.45716102,  0.49591469,  0.49203824,  0.4685821 ,\n",
       "         0.45745844,  0.45378732,  0.45893335,  0.45025861,  0.45424823,\n",
       "         0.51039026,  0.46583873,  0.46882628]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.steps[-1][1].components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "for i in model.steps[-1][1].components_:\n",
    "    f=i.argsort()[:-(2 - 1): -1]\n",
    "    lst.append(f)\n",
    "    print f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([], dtype=int64), array([], dtype=int64), array([], dtype=int64)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Recreating what I had done before losing everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings['description'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings['description'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = listings['description'].fillna('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_vectorized = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
      "             evaluate_every=-1, learning_decay=0.7,\n",
      "             learning_method='online', learning_offset=10.0,\n",
      "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
      "             n_components=10, n_jobs=-1, n_topics=20, perp_tol=0.1,\n",
      "             random_state=100, topic_word_prior=None,\n",
      "             total_samples=1000000.0, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "lda_model = LatentDirichletAllocation(n_topics=20,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Log Likelihood: ', -3210132.0502824672)\n",
      "('Perplexity: ', 851.78520451912868)\n",
      "{'learning_offset': 10.0, 'n_jobs': -1, 'topic_word_prior': None, 'perp_tol': 0.1, 'evaluate_every': -1, 'max_iter': 10, 'mean_change_tol': 0.001, 'batch_size': 128, 'max_doc_update_iter': 100, 'learning_decay': 0.7, 'n_components': 10, 'random_state': 100, 'doc_topic_prior': None, 'n_topics': 20, 'total_samples': 1000000.0, 'learning_method': 'online', 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "print(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Define Search Param\\nsearch_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\\n\\n# Init the Model\\nlda = LatentDirichletAllocation()\\n\\n# Init Grid Search Class\\nmodel = GridSearchCV(lda, param_grid=search_params)\\n\\n# Do the Grid Search\\nmodel.fit(data_vectorized)\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Best Model - using original lda_mdoel for now\\nbest_lda_model = model.best_estimator_\\nbest_lda_model = lda_model\\n# Model Parameters\\nprint(\"Best Model\\'s Params: \", model.best_params_)\\n\\n# Log Likelihood Score\\nprint(\"Best Log Likelihood Score: \", model.best_score_)\\n\\n# Perplexity\\nprint(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Best Model - using original lda_mdoel for now\n",
    "best_lda_model = model.best_estimator_\n",
    "best_lda_model = lda_model\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = lda_model.transform(data_vectorized)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_topics)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(corpus))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic0</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>Topic11</th>\n",
       "      <th>Topic12</th>\n",
       "      <th>Topic13</th>\n",
       "      <th>Topic14</th>\n",
       "      <th>Topic15</th>\n",
       "      <th>Topic16</th>\n",
       "      <th>Topic17</th>\n",
       "      <th>Topic18</th>\n",
       "      <th>Topic19</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic0  Topic1  Topic2  Topic3  Topic4  Topic5  Topic6  Topic7  Topic8  \\\n",
       "Doc0    0.00     0.0    0.00    0.86    0.00    0.00    0.00    0.00    0.00   \n",
       "Doc1    0.07     0.0    0.00    0.32    0.00    0.17    0.00    0.00    0.08   \n",
       "Doc2    0.00     0.0    0.31    0.25    0.00    0.00    0.00    0.00    0.00   \n",
       "Doc3    0.00     0.0    0.04    0.80    0.00    0.00    0.00    0.00    0.00   \n",
       "Doc4    0.00     0.0    0.01    0.60    0.01    0.16    0.02    0.02    0.00   \n",
       "\n",
       "      Topic9       ...        Topic11  Topic12  Topic13  Topic14  Topic15  \\\n",
       "Doc0    0.00       ...            0.0     0.00      0.0     0.00      0.0   \n",
       "Doc1    0.16       ...            0.0     0.00      0.0     0.00      0.0   \n",
       "Doc2    0.00       ...            0.0     0.05      0.0     0.00      0.0   \n",
       "Doc3    0.00       ...            0.0     0.00      0.0     0.09      0.0   \n",
       "Doc4    0.07       ...            0.0     0.03      0.0     0.00      0.0   \n",
       "\n",
       "      Topic16  Topic17  Topic18  Topic19  dominant_topic  \n",
       "Doc0     0.04      0.0     0.00     0.00               3  \n",
       "Doc1     0.15      0.0     0.00     0.03               3  \n",
       "Doc2     0.00      0.0     0.00     0.00              10  \n",
       "Doc3     0.00      0.0     0.00     0.00               3  \n",
       "Doc4     0.00      0.0     0.01     0.00               3  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_document_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_document_topic.index = [i for i in range(len(df_document_topic))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = df_document_topic.merge(listings, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now adding some more basic features created on text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def create_txt_features(pdseries):\n",
    "\n",
    "    textLength = []\n",
    "    textWordsPerc = []\n",
    "    textPuncPerc = []\n",
    "    textDigitsPerc = []\n",
    "\n",
    "    for i in pdseries:\n",
    "        tokens = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        textLength.append(len(tokens))\n",
    "\n",
    "        if len(tokens)==0:\n",
    "            textWordsPerc.append(0)\n",
    "            textPuncPerc.append(0)\n",
    "            textDigitsPerc.append(0)\n",
    "\n",
    "        else:\n",
    "            textWordsPerc.append(len(i.split())/float(len(tokens)))\n",
    "            textPuncPerc.append(len(''.join(c for c in i if c in string.punctuation))/float(len(tokens)))\n",
    "            textDigitsPerc.append(len(''.join(c for c in i if c in string.digits))/float(len(tokens)))\n",
    "\n",
    "    return textLength, textWordsPerc, textPuncPerc, textDigitsPerc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(text)/len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content)/len(text)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text_features:\n",
    "    out[i]=out[i].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['space',\n",
       " 'description',\n",
       " 'experiences_offered',\n",
       " 'neighborhood_overview',\n",
       " 'notes',\n",
       " 'transit',\n",
       " 'access',\n",
       " 'interaction',\n",
       " 'house_rules']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "textLength, textWordsPerc, textPuncPerc, textDigitsPerc = create_txt_features(out['space'])\n",
    "out['space_textLength'] = textLength\n",
    "out['space_textWordsPerc'] = textWordsPerc\n",
    "out['space_textPuncPerc'] = textPuncPerc\n",
    "out['space_textDigitsPerc'] = textDigitsPerc\n",
    "out['space_diversity'] = pd.Series([lexical_diversity(i) for i in out['space']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textLength, textWordsPerc, textPuncPerc, textDigitsPerc = create_txt_features(out['description'])\n",
    "out['description_textLength'] = textLength\n",
    "out['description_textWordsPerc'] = textWordsPerc\n",
    "out['description_textPuncPerc'] = textPuncPerc\n",
    "out['description_textDigitsPerc'] = textDigitsPerc\n",
    "out['description_diversity'] = pd.Series([lexical_diversity(i) for i in out['description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textLength, textWordsPerc, textPuncPerc, textDigitsPerc = create_txt_features(out['experiences_offered'])\n",
    "out['experiences_offered_textLength'] = textLength\n",
    "out['experiences_offered_textWordsPerc'] = textWordsPerc\n",
    "out['experiences_offered_textPuncPerc'] = textPuncPerc\n",
    "out['experiences_offered_textDigitsPerc'] = textDigitsPerc\n",
    "out['experiences_offered_diversity'] = pd.Series([lexical_diversity(i) for i in out['experiences_offered']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textLength, textWordsPerc, textPuncPerc, textDigitsPerc = create_txt_features(out['neighborhood_overview'])\n",
    "out['neighborhood_overview_textLength'] = textLength\n",
    "out['neighborhood_overview_textWordsPerc'] = textWordsPerc\n",
    "out['neighborhood_overview_textPuncPerc'] = textPuncPerc\n",
    "out['neighborhood_overview_textDigitsPerc'] = textDigitsPerc\n",
    "out['neighborhood_overview_diversity'] = pd.Series([lexical_diversity(i) for i in out['neighborhood_overview']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textLength, textWordsPerc, textPuncPerc, textDigitsPerc = create_txt_features(out['notes'])\n",
    "out['notes_textLength'] = textLength\n",
    "out['notes_textWordsPerc'] = textWordsPerc\n",
    "out['notes_textPuncPerc'] = textPuncPerc\n",
    "out['notes_textDigitsPerc'] = textDigitsPerc\n",
    "out['notes_diversity'] = pd.Series([lexical_diversity(i) for i in out['notes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textLength, textWordsPerc, textPuncPerc, textDigitsPerc = create_txt_features(out['transit'])\n",
    "out['transit_textLength'] = textLength\n",
    "out['transit_textWordsPerc'] = textWordsPerc\n",
    "out['transit_textPuncPerc'] = textPuncPerc\n",
    "out['transit_textDigitsPerc'] = textDigitsPerc\n",
    "out['transit_diversity'] = pd.Series([lexical_diversity(i) for i in out['transit']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textLength, textWordsPerc, textPuncPerc, textDigitsPerc = create_txt_features(out['access'])\n",
    "out['access_textLength'] = textLength\n",
    "out['access_textWordsPerc'] = textWordsPerc\n",
    "out['access_textPuncPerc'] = textPuncPerc\n",
    "out['access_textDigitsPerc'] = textDigitsPerc\n",
    "out['access_diversity'] = pd.Series([lexical_diversity(i) for i in out['access']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textLength, textWordsPerc, textPuncPerc, textDigitsPerc = create_txt_features(out['interaction'])\n",
    "out['interaction_textLength'] = textLength\n",
    "out['interaction_textWordsPerc'] = textWordsPerc\n",
    "out['interaction_textPuncPerc'] = textPuncPerc\n",
    "out['interaction_textDigitsPerc'] = textDigitsPerc\n",
    "out['interaction_diversity'] = pd.Series([lexical_diversity(i) for i in out['interaction']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textLength, textWordsPerc, textPuncPerc, textDigitsPerc = create_txt_features(out['house_rules'])\n",
    "out['house_rules_textLength'] = textLength\n",
    "out['house_rules_textWordsPerc'] = textWordsPerc\n",
    "out['house_rules_textPuncPerc'] = textPuncPerc\n",
    "out['house_rules_textDigitsPerc'] = textDigitsPerc\n",
    "out['house_rules_diversity'] = pd.Series([lexical_diversity(i) for i in out['house_rules']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.to_csv('listings_withtopics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
