{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation Notebook Consolidating All Data Prep code into linear process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from StringIO import StringIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings_original = pd.read_csv('Datasources/inside_airbnb/listings.csv')\n",
    "calendar_original = pd.read_csv('Datasources/inside_airbnb/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_columns(listings, cols):\n",
    "    chars = \"%$\"\n",
    "    for i in cols:\n",
    "        listings[i].astype(str).map(lambda x: x.rstrip(chars))\n",
    "        listings[i] = listings[i].apply(pd.to_numeric, errors='coerce')\n",
    "        listings[i].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "    return listings        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings = parse_columns(listings_original, ['host_response_rate', 'cleaning_fee',\\\n",
    "                                     'host_acceptance_rate','extra_people',\\\n",
    "                                     'weekly_price', 'monthly_price', 'security_deposit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to produce 4 listings dataframes (whole, holiday, wke, wkd) with listing mean price\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "def get_mean_price(cal, listings):\n",
    "    \n",
    "    cal['price'] = cal['price'].astype(str).map(lambda x: x.lstrip('$'))\n",
    "    cal['price'] = cal['price'].apply(pd.to_numeric, errors='coerce')\n",
    "    cal['price'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "    cal['date'] = pd.to_datetime(cal['date'])\n",
    "    cal['month'] = cal['date'].apply(lambda x: x.month)\n",
    "    cal['day'] = cal['date'].apply(lambda x: x.day)\n",
    "    cal['day_of_week'] = cal['date'].dt.weekday_name\n",
    "    \n",
    "    cl = calendar()\n",
    "    holidays = cl.holidays(start=cal['date'].min(), end=cal['date'].max())\n",
    "    \n",
    "    cal['holiday'] = cal['date'].isin(holidays)\n",
    "    cal = cal[(cal['date']>'2016-07-06')&(cal['date']<'2016-10-06')]\n",
    "    \n",
    "    c = cal.loc[cal.available!='f']\n",
    "    c = c[['listing_id','date','price','month','day_of_week','holiday']]\n",
    "    c=c.fillna(c.mean())\n",
    "    \n",
    "    c_hol = c[c['holiday']==True]\n",
    "    c_wke = c[(c['holiday']==False)&((c['day_of_week']=='Sunday')|(c['day_of_week']=='Saturday'))]\n",
    "    c_wkd = c[(~c.isin(c_hol['date']))&(~c.isin(c_wke['date']))]\n",
    "\n",
    "\n",
    "    price_hol_dict = {'price': c_hol.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c_hol.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c_hol.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c_hol.groupby(by='listing_id')['price'].fillna(0).std(),                 \n",
    "                  'skew_of_price': c_hol.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                     'median_price': c_hol.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "    price_wke_dict = {'price': c_wke.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c_wke.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c_wke.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c_wke.groupby(by='listing_id')['price'].fillna(0).std(),\n",
    "                  'skew_of_price': c_wke.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                     'median_price': c_wke.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "    price_wkd_dict = {'price': c_wkd.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c_wkd.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c_wkd.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c_wkd.groupby(by='listing_id')['price'].fillna(0).std(),\n",
    "                  'skew_of_price': c_wkd.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                     'median_price': c_wkd.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "    price_whole_dict = {'price': c.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c.groupby(by='listing_id')['price'].fillna(0).std(),\n",
    "                  'skew_of_price': c.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                       'median_price': c.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "\n",
    "    price_hol = pd.DataFrame(price_hol_dict)\n",
    "    price_wke = pd.DataFrame(price_wke_dict)\n",
    "    price_wkd = pd.DataFrame(price_wkd_dict)\n",
    "    price_c = pd.DataFrame(price_whole_dict)    \n",
    "    \n",
    "    price_hol = price_hol.reset_index()\n",
    "    price_wke = price_wke.reset_index()\n",
    "    price_wkd = price_wkd.reset_index()\n",
    "    price_c = price_c.reset_index()\n",
    "\n",
    "    listings_hol = listings.merge(price_hol, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_wke = listings.merge(price_wke, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_wkd = listings.merge(price_wkd, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_c = listings.merge(price_c, how='inner', left_on='id', right_on='listing_id')\n",
    "    \n",
    "    price_hol_new = price_hol.rename(columns = {'max_price': 'hol_max_price', 'min_price': 'hol_min_price', 'price': 'hol_price',\n",
    "                                           'skew_of_price': 'hol_skew_of_price', 'stdev_of_price': 'hol_stdev_of_price',\n",
    "                                               'median_price' : 'hol_median_price'})\n",
    "    price_wke_new = price_wke.rename(columns = {'max_price': 'wke_max_price', 'min_price': 'wke_min_price', 'price': 'wke_price',\n",
    "                                           'skew_of_price': 'wke_skew_of_price', 'stdev_of_price': 'wke_stdev_of_price',\n",
    "                                               'median_price' : 'wke_median_price'})\n",
    "    price_wkd_new = price_wkd.rename(columns = {'max_price': 'wkd_max_price', 'min_price': 'wkd_min_price', 'price': 'wkd_price',\n",
    "                                           'skew_of_price': 'wkd_skew_of_price', 'stdev_of_price': 'wkd_stdev_of_price',\n",
    "                                               'median_price' : 'wkd_median_price'})\n",
    "    \n",
    "    listings_c = listings_c.merge(price_hol_new, how='outer', left_on='id', right_on='listing_id')\n",
    "    listings_c = listings_c.merge(price_wke_new, how='outer', left_on='id', right_on='listing_id')\n",
    "    listings_c = listings_c.merge(price_wkd_new, how='outer', left_on='id', right_on='listing_id')\n",
    "    \n",
    "    #len(cal['listing_id'].astype(str).unique())\n",
    "    #count = len(c['listing_id'].astype(str).unique())\n",
    "    \n",
    "    #print('Due to the above filtering on calendar, the right total count of listings is: ' %(count))\n",
    "    \n",
    "    return listings_hol, listings_wke, listings_wkd, listings_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings_hol, listings_wke, listings_wkd, listings = get_mean_price(calendar_original, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use n components in place of n topics when using gridsearchcv\n",
    "def create_topics(pdseries, listings):\n",
    "        corpus = pdseries.fillna('none')\n",
    "        \n",
    "        vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "        \n",
    "        data_vectorized = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        lda_model = LatentDirichletAllocation(n_topics=20,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "        \n",
    "        lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "        # column names\n",
    "        col_name = pd.DataFrame(pdseries).columns[0]\n",
    "        topicnames = [str(col_name) + \"-\" + \"Topic\" + str(i) for i in range(lda_model.n_topics)]\n",
    "\n",
    "        # index names\n",
    "        docnames = [str(col_name) + \"-\" + \"Doc\" + str(i) for i in range(len(corpus))]\n",
    "\n",
    "        # Make the pandas dataframe\n",
    "        df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "        # Get dominant topic for each document\n",
    "        dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "        df_document_topic[str(col_name) + \"-\" + 'Dominant_Topic'] = dominant_topic\n",
    "        \n",
    "        df_document_topic.index = [i for i in range(len(df_document_topic))]\n",
    "        \n",
    "        df_document_topic = df_document_topic.fillna(0)\n",
    "        \n",
    "        out = df_document_topic.merge(listings, left_index=True, right_index=True)\n",
    "        out = out.astype('str')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new = listings.copy()\n",
    "for i in text_features:\n",
    "    new = create_topics(listings[i], new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def create_txt_features(pdseries, listings):\n",
    "    \n",
    "    textLength = []\n",
    "    textWordsPerc = []\n",
    "    textPuncPerc = []\n",
    "    textDigitsPerc = []\n",
    "\n",
    "    for i in pdseries:\n",
    "        tokens = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        textLength.append(len(tokens))\n",
    "\n",
    "        if len(tokens)==0:\n",
    "            textWordsPerc.append(0)\n",
    "            textPuncPerc.append(0)\n",
    "            textDigitsPerc.append(0)\n",
    "\n",
    "        else:\n",
    "            textWordsPerc.append(len(i.split())/float(len(tokens)))\n",
    "            textPuncPerc.append(len(''.join(c for c in i if c in string.punctuation))/float(len(tokens)))\n",
    "            textDigitsPerc.append(len(''.join(c for c in i if c in string.digits))/float(len(tokens)))\n",
    "\n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    \n",
    "    textLength_varname = str(col_name) + '_TextLength'\n",
    "    textWordsPerc_varname = str(col_name) + '_TextWordsPerc'\n",
    "    textPuncPerc_varname = str(col_name) + '_TextPuncPerc'\n",
    "    textDigitsPerc_varname = str(col_name) + '_TextDigitsPerc'\n",
    "    \n",
    "    listings[textLength_varname] = textLength\n",
    "    listings[textWordsPerc_varname] = textWordsPerc\n",
    "    listings[textPuncPerc_varname] = textPuncPerc\n",
    "    listings[textDigitsPerc_varname] = textDigitsPerc\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new2 = new.copy()\n",
    "for i in text_features:\n",
    "    new2 = create_txt_features(new[i], new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(pdseries, listings):\n",
    "    \n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    varname = str(col_name) + \"_LexicalDiversity\"\n",
    "    \n",
    "    lx_div = pd.Series([len(i)/len(set(i)) for i in pdseries])\n",
    "    listings[varname] = lx_div\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new3 = new2.copy()\n",
    "for i in text_features:\n",
    "    new3 = lexical_diversity(new2[i], new3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_grammar(pdseries, listings):\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.tag import pos_tag, map_tag\n",
    "    from collections import Counter\n",
    "      \n",
    "    df = pd.DataFrame()\n",
    "    for text in pdseries:\n",
    "        \n",
    "        col_name = pd.DataFrame(pdseries).columns[0]\n",
    "        \n",
    "        \n",
    "        tokenized_text = nltk.word_tokenize(text.decode('utf-8'))\n",
    "        grammar = [i[1] for i in nltk.pos_tag(tokenized_text, tagset='universal')]\n",
    "        \n",
    "        counter = Counter(grammar)\n",
    "        fr = pd.DataFrame(counter, index=[0])\n",
    "        fr.columns = [str(col_name) + '_' + str(i) for i in fr.columns]\n",
    "        \n",
    "        fr2 = fr/len(tokenized_text)\n",
    "        fr2.columns = [str(i) + '_tokens_sum_ratio' for i in fr2.columns]\n",
    "        \n",
    "        fr3 = pd.concat([fr, fr2], ignore_index=True)\n",
    "        \n",
    "        df = pd.concat([df, fr3], ignore_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    df = df.fillna(0)\n",
    "        \n",
    "    return listings.merge(df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new4 = new3.copy()\n",
    "for i in text_features:\n",
    "    new4 = extract_grammar(new3[i], new4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_Clusterer(pdseries, listings):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(pdseries)\n",
    "    true_k = 10\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "    model.fit(X)\n",
    "    \n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    varname = str(col_name) + \"_KmeansCluster\"\n",
    "    \n",
    "    listings[varname] = pd.Series(model.labels_)\n",
    "    listings[varname] = listings[varname].fillna(0)\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new5 = new4.copy()\n",
    "for i in text_features:\n",
    "    new5 = kmeans_Clusterer(new4[i], new5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_to_set(x):\n",
    "    c = set()\n",
    "    for w in x[1:-1].split(\",\"):\n",
    "        c.add(w)\n",
    "        \n",
    "    return c\n",
    "\n",
    "def has_amenity(x, amen_):\n",
    "    if amen_ in x:\n",
    "        return 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_amenities(listings):\n",
    "    listings['amenities_set'] = listings['amenities'].fillna('{}').map(string_to_set)\n",
    "    all_amenities = set()\n",
    "    \n",
    "    for idx in listings['amenities'].fillna('{}').map(string_to_set).index:\n",
    "        all_amenities = all_amenities.union(listings['amenities'].fillna('{}').map(string_to_set)[idx])\n",
    "        \n",
    "    for amen in all_amenities:\n",
    "        listings['has' + amen] = 0\n",
    "        listings['has' + amen] = listings['amenities_set'].map(lambda x: has_amenity(x, amen))\n",
    "\n",
    "    has_amenties_list = []\n",
    "    for amen in all_amenities:\n",
    "        has_amenties_list.append('has' + amen)\n",
    "\n",
    "    listings[has_amenties_list] = listings[has_amenties_list].fillna(0)\n",
    "    \n",
    "    return listings      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new6 = new5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new6 = add_amenities(new6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_host_verifications(listings):\n",
    "    a = listings['host_verifications'].map(lambda x: x[1:-1]).map(lambda j: j.split(',')).map(lambda k: set(k))\n",
    "    all_host_verifications = set()\n",
    "    \n",
    "    for w in a.index:\n",
    "        all_host_verifications = all_host_verifications.union(a[w])\n",
    "        \n",
    "    for w in all_host_verifications:\n",
    "        listings['uses' + w] = 0\n",
    "        listings['uses' + w] = a.map(lambda x: has_amenity(x, w))\n",
    "    \n",
    "    \n",
    "    uses_verification_list = []\n",
    "    \n",
    "    for veri in all_host_verifications:\n",
    "        uses_verification_list.append('uses' + veri)\n",
    "    \n",
    "    listings[uses_verification_list] = listings[uses_verification_list].fillna(0)\n",
    "    \n",
    "    return listings      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new7 = new6.copy()\n",
    "new7 = add_host_verifications(new7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_distance_from_ocean(listings):\n",
    "    listings['distance_from_ocean'] = 0\n",
    "    listings['distance_from_ocean'] = listings['distance_from_ocean'].astype('float')\n",
    "    \n",
    "    for w in listings.index:\n",
    "        p = float(listings['latitude'][w])\n",
    "        q = float(listings['longitude'][w])\n",
    "        lon_diff = (q + 117.235585)*np.pi/180\n",
    "        lat_diff = (p - 32.802458)*np.pi/180\n",
    "        a = np.sin(lat_diff/2)**2 + np.cos(p*np.pi/180)*np.cos(32.802458*np.pi/180)*(np.sin(lon_diff/2)**2)\n",
    "        c = np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        d = 6371.00*float(c)\n",
    "        listings['distance_from_ocean'][w] = d\n",
    "        \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new8 = new7.copy()\n",
    "new8 = add_distance_from_ocean(new8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(listings, encoded_features):\n",
    "    \n",
    "    label_enc = LabelEncoder()\n",
    "    \n",
    "    for col in encoded_features:\n",
    "        \n",
    "        listings[col] = listings[col].astype(str)\n",
    "        \n",
    "        var_name = str(col) + '_enc'\n",
    "        listings[var_name] = label_enc.fit_transform(listings[col])\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_vars = ['host_response_time', 'calendar_updated', 'bed_type', 'jurisdiction_names', 'zipcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new9 = new8.copy()\n",
    "new9 = encoder(new9, encoded_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Caution!!! The input features are not dropped by the following to columns - they must be dropped as part of modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarizer(listings, binarized_features):\n",
    "    \n",
    "    label_enc = LabelBinarizer()\n",
    "    \n",
    "    for col in binarized_features:\n",
    "        \n",
    "        listings[col] = listings[col].astype(str)\n",
    "        \n",
    "        var_name = str(col) + '_bin'\n",
    "        listings[var_name] = label_enc.fit_transform(listings[col])\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binarized_vars = ['host_is_superhost','is_location_exact','host_has_profile_pic','host_identity_verified',\n",
    "                  'instant_bookable','require_guest_profile_picture','require_guest_phone_verification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new10 = new9.copy()\n",
    "new10 = binarizer(new10, binarized_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes list of features that should be numeric and transforms them to float\n",
    "#Also takes care of the topic features - these need not be input into the features parameter\n",
    "def make_numeric(listings, features):\n",
    "    #Taking Care of topics features\n",
    "    topic_cols = listings.filter(regex='Topic').columns\n",
    "    listings[topic_cols] = listings[topic_cols].astype(float)\n",
    "    \n",
    "    #Now transforming the other features:\n",
    "    for i in features:\n",
    "        listings[i] = listings[i].astype(float)\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings.wke_max_price.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_columns_new(listings, cols):\n",
    "    \n",
    "    topic_cols = listings.filter(regex='Topic').columns\n",
    "    listings[topic_cols] = listings[topic_cols].astype(float)\n",
    "    \n",
    "    for i in cols:\n",
    "        listings[i] = listings[i].replace('$', '')\n",
    "        #listings[i] = listings[i].replace('%', '')\n",
    "        listings[i] = listings[i].apply(pd.to_numeric, errors='coerce')\n",
    "        listings[i].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = ['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', \n",
    "               'beds', 'guests_included', 'minimum_nights',\n",
    "               'maximum_nights', 'availability_30', 'availability_60','availability_90',\n",
    "               'availability_365', 'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy',\n",
    "               'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n",
    "               'review_scores_location', 'review_scores_value', 'calculated_host_listings_count', \n",
    "               'reviews_per_month', 'max_price', 'median_price_x', 'min_price', 'price_y', 'skew_of_price',\n",
    "               'stdev_of_price', 'hol_max_price', 'median_price_y', 'hol_min_price', 'hol_price', 'hol_skew_of_price', \n",
    "               'hol_stdev_of_price', 'wke_max_price', 'wke_min_price', 'wke_price', 'wke_skew_of_price', 'wke_stdev_of_price',\n",
    "               'wkd_max_price', 'wkd_min_price', 'wkd_skew_of_price', 'wkd_stdev_of_price', 'space_KmeansCluster', \n",
    "               'description_KmeansCluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'median_price_x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-984ebc96a54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_columns_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#new11 = make_numeric(new11, num_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-782d2251040c>\u001b[0m in \u001b[0;36mparse_columns_new\u001b[0;34m(listings, cols)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mlistings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m#listings[i] = listings[i].replace('%', '')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlistings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2151\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'median_price_x'"
     ]
    }
   ],
   "source": [
    "new11 = new10.copy()\n",
    "new11 = parse_columns_new(new11, num_features)\n",
    "#new11 = make_numeric(new11, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house_rules-Topic0\n",
      "house_rules-Topic1\n",
      "house_rules-Topic2\n",
      "house_rules-Topic3\n",
      "house_rules-Topic4\n",
      "house_rules-Topic5\n",
      "house_rules-Topic6\n",
      "house_rules-Topic7\n",
      "house_rules-Topic8\n",
      "house_rules-Topic9\n",
      "house_rules-Topic10\n",
      "house_rules-Topic11\n",
      "house_rules-Topic12\n",
      "house_rules-Topic13\n",
      "house_rules-Topic14\n",
      "house_rules-Topic15\n",
      "house_rules-Topic16\n",
      "house_rules-Topic17\n",
      "house_rules-Topic18\n",
      "house_rules-Topic19\n",
      "house_rules-Dominant_Topic\n",
      "interaction-Topic0\n",
      "interaction-Topic1\n",
      "interaction-Topic2\n",
      "interaction-Topic3\n",
      "interaction-Topic4\n",
      "interaction-Topic5\n",
      "interaction-Topic6\n",
      "interaction-Topic7\n",
      "interaction-Topic8\n",
      "interaction-Topic9\n",
      "interaction-Topic10\n",
      "interaction-Topic11\n",
      "interaction-Topic12\n",
      "interaction-Topic13\n",
      "interaction-Topic14\n",
      "interaction-Topic15\n",
      "interaction-Topic16\n",
      "interaction-Topic17\n",
      "interaction-Topic18\n",
      "interaction-Topic19\n",
      "interaction-Dominant_Topic\n",
      "access-Topic0\n",
      "access-Topic1\n",
      "access-Topic2\n",
      "access-Topic3\n",
      "access-Topic4\n",
      "access-Topic5\n",
      "access-Topic6\n",
      "access-Topic7\n",
      "access-Topic8\n",
      "access-Topic9\n",
      "access-Topic10\n",
      "access-Topic11\n",
      "access-Topic12\n",
      "access-Topic13\n",
      "access-Topic14\n",
      "access-Topic15\n",
      "access-Topic16\n",
      "access-Topic17\n",
      "access-Topic18\n",
      "access-Topic19\n",
      "access-Dominant_Topic\n",
      "transit-Topic0\n",
      "transit-Topic1\n",
      "transit-Topic2\n",
      "transit-Topic3\n",
      "transit-Topic4\n",
      "transit-Topic5\n",
      "transit-Topic6\n",
      "transit-Topic7\n",
      "transit-Topic8\n",
      "transit-Topic9\n",
      "transit-Topic10\n",
      "transit-Topic11\n",
      "transit-Topic12\n",
      "transit-Topic13\n",
      "transit-Topic14\n",
      "transit-Topic15\n",
      "transit-Topic16\n",
      "transit-Topic17\n",
      "transit-Topic18\n",
      "transit-Topic19\n",
      "transit-Dominant_Topic\n",
      "notes-Topic0\n",
      "notes-Topic1\n",
      "notes-Topic2\n",
      "notes-Topic3\n",
      "notes-Topic4\n",
      "notes-Topic5\n",
      "notes-Topic6\n",
      "notes-Topic7\n",
      "notes-Topic8\n",
      "notes-Topic9\n",
      "notes-Topic10\n",
      "notes-Topic11\n",
      "notes-Topic12\n",
      "notes-Topic13\n",
      "notes-Topic14\n",
      "notes-Topic15\n",
      "notes-Topic16\n",
      "notes-Topic17\n",
      "notes-Topic18\n",
      "notes-Topic19\n",
      "notes-Dominant_Topic\n",
      "neighborhood_overview-Topic0\n",
      "neighborhood_overview-Topic1\n",
      "neighborhood_overview-Topic2\n",
      "neighborhood_overview-Topic3\n",
      "neighborhood_overview-Topic4\n",
      "neighborhood_overview-Topic5\n",
      "neighborhood_overview-Topic6\n",
      "neighborhood_overview-Topic7\n",
      "neighborhood_overview-Topic8\n",
      "neighborhood_overview-Topic9\n",
      "neighborhood_overview-Topic10\n",
      "neighborhood_overview-Topic11\n",
      "neighborhood_overview-Topic12\n",
      "neighborhood_overview-Topic13\n",
      "neighborhood_overview-Topic14\n",
      "neighborhood_overview-Topic15\n",
      "neighborhood_overview-Topic16\n",
      "neighborhood_overview-Topic17\n",
      "neighborhood_overview-Topic18\n",
      "neighborhood_overview-Topic19\n",
      "neighborhood_overview-Dominant_Topic\n",
      "description-Topic0\n",
      "description-Topic1\n",
      "description-Topic2\n",
      "description-Topic3\n",
      "description-Topic4\n",
      "description-Topic5\n",
      "description-Topic6\n",
      "description-Topic7\n",
      "description-Topic8\n",
      "description-Topic9\n",
      "description-Topic10\n",
      "description-Topic11\n",
      "description-Topic12\n",
      "description-Topic13\n",
      "description-Topic14\n",
      "description-Topic15\n",
      "description-Topic16\n",
      "description-Topic17\n",
      "description-Topic18\n",
      "description-Topic19\n",
      "description-Dominant_Topic\n",
      "space-Topic0\n",
      "space-Topic1\n",
      "space-Topic2\n",
      "space-Topic3\n",
      "space-Topic4\n",
      "space-Topic5\n",
      "space-Topic6\n",
      "space-Topic7\n",
      "space-Topic8\n",
      "space-Topic9\n",
      "space-Topic10\n",
      "space-Topic11\n",
      "space-Topic12\n",
      "space-Topic13\n",
      "space-Topic14\n",
      "space-Topic15\n",
      "space-Topic16\n",
      "space-Topic17\n",
      "space-Topic18\n",
      "space-Topic19\n",
      "space-Dominant_Topic\n",
      "id\n",
      "listing_url\n",
      "scrape_id\n",
      "last_scraped\n",
      "name\n",
      "summary\n",
      "space\n",
      "description\n",
      "experiences_offered\n",
      "neighborhood_overview\n",
      "notes\n",
      "transit\n",
      "access\n",
      "interaction\n",
      "house_rules\n",
      "thumbnail_url\n",
      "medium_url\n",
      "picture_url\n",
      "xl_picture_url\n",
      "host_id\n",
      "host_url\n",
      "host_name\n",
      "host_since\n",
      "host_location\n",
      "host_about\n",
      "host_response_time\n",
      "host_response_rate\n",
      "host_acceptance_rate\n",
      "host_is_superhost\n",
      "host_thumbnail_url\n",
      "host_picture_url\n",
      "host_neighbourhood\n",
      "host_listings_count\n",
      "host_total_listings_count\n",
      "host_verifications\n",
      "host_has_profile_pic\n",
      "host_identity_verified\n",
      "street\n",
      "neighbourhood\n",
      "neighbourhood_cleansed\n",
      "neighbourhood_group_cleansed\n",
      "city\n",
      "state\n",
      "zipcode\n",
      "market\n",
      "smart_location\n",
      "country_code\n",
      "country\n",
      "latitude\n",
      "longitude\n",
      "is_location_exact\n",
      "property_type\n",
      "room_type\n",
      "accommodates\n",
      "bathrooms\n",
      "bedrooms\n",
      "beds\n",
      "bed_type\n",
      "amenities\n",
      "square_feet\n",
      "price_x\n",
      "weekly_price\n",
      "monthly_price\n",
      "security_deposit\n",
      "cleaning_fee\n",
      "guests_included\n",
      "extra_people\n",
      "minimum_nights\n",
      "maximum_nights\n",
      "calendar_updated\n",
      "has_availability\n",
      "availability_30\n",
      "availability_60\n",
      "availability_90\n",
      "availability_365\n",
      "calendar_last_scraped\n",
      "number_of_reviews\n",
      "first_review\n",
      "last_review\n",
      "review_scores_rating\n",
      "review_scores_accuracy\n",
      "review_scores_cleanliness\n",
      "review_scores_checkin\n",
      "review_scores_communication\n",
      "review_scores_location\n",
      "review_scores_value\n",
      "requires_license\n",
      "license\n",
      "jurisdiction_names\n",
      "instant_bookable\n",
      "cancellation_policy\n",
      "require_guest_profile_picture\n",
      "require_guest_phone_verification\n",
      "calculated_host_listings_count\n",
      "reviews_per_month\n",
      "listing_id_x\n",
      "max_price\n",
      "median_price\n",
      "min_price\n",
      "price_y\n",
      "skew_of_price\n",
      "stdev_of_price\n",
      "listing_id_y\n",
      "hol_max_price\n",
      "hol_median_price\n",
      "hol_min_price\n",
      "hol_price\n",
      "hol_skew_of_price\n",
      "hol_stdev_of_price\n",
      "listing_id_x\n",
      "wke_max_price\n",
      "wke_median_price\n",
      "wke_min_price\n",
      "wke_price\n",
      "wke_skew_of_price\n",
      "wke_stdev_of_price\n",
      "listing_id_y\n",
      "wkd_max_price\n",
      "wkd_median_price\n",
      "wkd_min_price\n",
      "wkd_price\n",
      "wkd_skew_of_price\n",
      "wkd_stdev_of_price\n",
      "space_TextLength\n",
      "space_TextWordsPerc\n",
      "space_TextPuncPerc\n",
      "space_TextDigitsPerc\n",
      "description_TextLength\n",
      "description_TextWordsPerc\n",
      "description_TextPuncPerc\n",
      "description_TextDigitsPerc\n",
      "neighborhood_overview_TextLength\n",
      "neighborhood_overview_TextWordsPerc\n",
      "neighborhood_overview_TextPuncPerc\n",
      "neighborhood_overview_TextDigitsPerc\n",
      "notes_TextLength\n",
      "notes_TextWordsPerc\n",
      "notes_TextPuncPerc\n",
      "notes_TextDigitsPerc\n",
      "transit_TextLength\n",
      "transit_TextWordsPerc\n",
      "transit_TextPuncPerc\n",
      "transit_TextDigitsPerc\n",
      "access_TextLength\n",
      "access_TextWordsPerc\n",
      "access_TextPuncPerc\n",
      "access_TextDigitsPerc\n",
      "interaction_TextLength\n",
      "interaction_TextWordsPerc\n",
      "interaction_TextPuncPerc\n",
      "interaction_TextDigitsPerc\n",
      "house_rules_TextLength\n",
      "house_rules_TextWordsPerc\n",
      "house_rules_TextPuncPerc\n",
      "house_rules_TextDigitsPerc\n",
      "space_LexicalDiversity\n",
      "description_LexicalDiversity\n",
      "neighborhood_overview_LexicalDiversity\n",
      "notes_LexicalDiversity\n",
      "transit_LexicalDiversity\n",
      "access_LexicalDiversity\n",
      "interaction_LexicalDiversity\n",
      "house_rules_LexicalDiversity\n",
      "space_.\n",
      "space_._tokens_sum_ratio\n",
      "space_ADJ\n",
      "space_ADJ_tokens_sum_ratio\n",
      "space_ADP\n",
      "space_ADP_tokens_sum_ratio\n",
      "space_ADV\n",
      "space_ADV_tokens_sum_ratio\n",
      "space_CONJ\n",
      "space_CONJ_tokens_sum_ratio\n",
      "space_DET\n",
      "space_DET_tokens_sum_ratio\n",
      "space_NOUN\n",
      "space_NOUN_tokens_sum_ratio\n",
      "space_NUM\n",
      "space_NUM_tokens_sum_ratio\n",
      "space_PRON\n",
      "space_PRON_tokens_sum_ratio\n",
      "space_PRT\n",
      "space_PRT_tokens_sum_ratio\n",
      "space_VERB\n",
      "space_VERB_tokens_sum_ratio\n",
      "space_X\n",
      "space_X_tokens_sum_ratio\n",
      "description_.\n",
      "description_._tokens_sum_ratio\n",
      "description_ADJ\n",
      "description_ADJ_tokens_sum_ratio\n",
      "description_ADP\n",
      "description_ADP_tokens_sum_ratio\n",
      "description_ADV\n",
      "description_ADV_tokens_sum_ratio\n",
      "description_CONJ\n",
      "description_CONJ_tokens_sum_ratio\n",
      "description_DET\n",
      "description_DET_tokens_sum_ratio\n",
      "description_NOUN\n",
      "description_NOUN_tokens_sum_ratio\n",
      "description_NUM\n",
      "description_NUM_tokens_sum_ratio\n",
      "description_PRON\n",
      "description_PRON_tokens_sum_ratio\n",
      "description_PRT\n",
      "description_PRT_tokens_sum_ratio\n",
      "description_VERB\n",
      "description_VERB_tokens_sum_ratio\n",
      "description_X\n",
      "description_X_tokens_sum_ratio\n",
      "neighborhood_overview_.\n",
      "neighborhood_overview_._tokens_sum_ratio\n",
      "neighborhood_overview_ADJ\n",
      "neighborhood_overview_ADJ_tokens_sum_ratio\n",
      "neighborhood_overview_ADP\n",
      "neighborhood_overview_ADP_tokens_sum_ratio\n",
      "neighborhood_overview_ADV\n",
      "neighborhood_overview_ADV_tokens_sum_ratio\n",
      "neighborhood_overview_CONJ\n",
      "neighborhood_overview_CONJ_tokens_sum_ratio\n",
      "neighborhood_overview_DET\n",
      "neighborhood_overview_DET_tokens_sum_ratio\n",
      "neighborhood_overview_NOUN\n",
      "neighborhood_overview_NOUN_tokens_sum_ratio\n",
      "neighborhood_overview_NUM\n",
      "neighborhood_overview_NUM_tokens_sum_ratio\n",
      "neighborhood_overview_PRON\n",
      "neighborhood_overview_PRON_tokens_sum_ratio\n",
      "neighborhood_overview_PRT\n",
      "neighborhood_overview_PRT_tokens_sum_ratio\n",
      "neighborhood_overview_VERB\n",
      "neighborhood_overview_VERB_tokens_sum_ratio\n",
      "neighborhood_overview_X\n",
      "neighborhood_overview_X_tokens_sum_ratio\n",
      "notes_.\n",
      "notes_._tokens_sum_ratio\n",
      "notes_ADJ\n",
      "notes_ADJ_tokens_sum_ratio\n",
      "notes_ADP\n",
      "notes_ADP_tokens_sum_ratio\n",
      "notes_ADV\n",
      "notes_ADV_tokens_sum_ratio\n",
      "notes_CONJ\n",
      "notes_CONJ_tokens_sum_ratio\n",
      "notes_DET\n",
      "notes_DET_tokens_sum_ratio\n",
      "notes_NOUN\n",
      "notes_NOUN_tokens_sum_ratio\n",
      "notes_NUM\n",
      "notes_NUM_tokens_sum_ratio\n",
      "notes_PRON\n",
      "notes_PRON_tokens_sum_ratio\n",
      "notes_PRT\n",
      "notes_PRT_tokens_sum_ratio\n",
      "notes_VERB\n",
      "notes_VERB_tokens_sum_ratio\n",
      "notes_X\n",
      "notes_X_tokens_sum_ratio\n",
      "transit_.\n",
      "transit_._tokens_sum_ratio\n",
      "transit_ADJ\n",
      "transit_ADJ_tokens_sum_ratio\n",
      "transit_ADP\n",
      "transit_ADP_tokens_sum_ratio\n",
      "transit_ADV\n",
      "transit_ADV_tokens_sum_ratio\n",
      "transit_CONJ\n",
      "transit_CONJ_tokens_sum_ratio\n",
      "transit_DET\n",
      "transit_DET_tokens_sum_ratio\n",
      "transit_NOUN\n",
      "transit_NOUN_tokens_sum_ratio\n",
      "transit_NUM\n",
      "transit_NUM_tokens_sum_ratio\n",
      "transit_PRON\n",
      "transit_PRON_tokens_sum_ratio\n",
      "transit_PRT\n",
      "transit_PRT_tokens_sum_ratio\n",
      "transit_VERB\n",
      "transit_VERB_tokens_sum_ratio\n",
      "transit_X\n",
      "transit_X_tokens_sum_ratio\n",
      "access_.\n",
      "access_._tokens_sum_ratio\n",
      "access_ADJ\n",
      "access_ADJ_tokens_sum_ratio\n",
      "access_ADP\n",
      "access_ADP_tokens_sum_ratio\n",
      "access_ADV\n",
      "access_ADV_tokens_sum_ratio\n",
      "access_CONJ\n",
      "access_CONJ_tokens_sum_ratio\n",
      "access_DET\n",
      "access_DET_tokens_sum_ratio\n",
      "access_NOUN\n",
      "access_NOUN_tokens_sum_ratio\n",
      "access_NUM\n",
      "access_NUM_tokens_sum_ratio\n",
      "access_PRON\n",
      "access_PRON_tokens_sum_ratio\n",
      "access_PRT\n",
      "access_PRT_tokens_sum_ratio\n",
      "access_VERB\n",
      "access_VERB_tokens_sum_ratio\n",
      "access_X\n",
      "access_X_tokens_sum_ratio\n",
      "interaction_.\n",
      "interaction_._tokens_sum_ratio\n",
      "interaction_ADJ\n",
      "interaction_ADJ_tokens_sum_ratio\n",
      "interaction_ADP\n",
      "interaction_ADP_tokens_sum_ratio\n",
      "interaction_ADV\n",
      "interaction_ADV_tokens_sum_ratio\n",
      "interaction_CONJ\n",
      "interaction_CONJ_tokens_sum_ratio\n",
      "interaction_DET\n",
      "interaction_DET_tokens_sum_ratio\n",
      "interaction_NOUN\n",
      "interaction_NOUN_tokens_sum_ratio\n",
      "interaction_NUM\n",
      "interaction_NUM_tokens_sum_ratio\n",
      "interaction_PRON\n",
      "interaction_PRON_tokens_sum_ratio\n",
      "interaction_PRT\n",
      "interaction_PRT_tokens_sum_ratio\n",
      "interaction_VERB\n",
      "interaction_VERB_tokens_sum_ratio\n",
      "interaction_X\n",
      "interaction_X_tokens_sum_ratio\n",
      "house_rules_.\n",
      "house_rules_._tokens_sum_ratio\n",
      "house_rules_ADJ\n",
      "house_rules_ADJ_tokens_sum_ratio\n",
      "house_rules_ADP\n",
      "house_rules_ADP_tokens_sum_ratio\n",
      "house_rules_ADV\n",
      "house_rules_ADV_tokens_sum_ratio\n",
      "house_rules_CONJ\n",
      "house_rules_CONJ_tokens_sum_ratio\n",
      "house_rules_DET\n",
      "house_rules_DET_tokens_sum_ratio\n",
      "house_rules_NOUN\n",
      "house_rules_NOUN_tokens_sum_ratio\n",
      "house_rules_NUM\n",
      "house_rules_NUM_tokens_sum_ratio\n",
      "house_rules_PRON\n",
      "house_rules_PRON_tokens_sum_ratio\n",
      "house_rules_PRT\n",
      "house_rules_PRT_tokens_sum_ratio\n",
      "house_rules_VERB\n",
      "house_rules_VERB_tokens_sum_ratio\n",
      "house_rules_X\n",
      "house_rules_X_tokens_sum_ratio\n",
      "space_KmeansCluster\n",
      "description_KmeansCluster\n",
      "neighborhood_overview_KmeansCluster\n",
      "notes_KmeansCluster\n",
      "transit_KmeansCluster\n",
      "access_KmeansCluster\n",
      "interaction_KmeansCluster\n",
      "house_rules_KmeansCluster\n",
      "amenities_set\n",
      "has\n",
      "has\"Other pet(s)\"\n",
      "hasEssentials\n",
      "has\"Carbon Monoxide Detector\"\n",
      "has\"Elevator in Building\"\n",
      "has\"Indoor Fireplace\"\n",
      "has\"translation missing: en.hosting_amenity_50\"\n",
      "hasInternet\n",
      "hasWasher\n",
      "hasHangers\n",
      "has\"Buzzer/Wireless Intercom\"\n",
      "hasTV\n",
      "hasGym\n",
      "has\"Fire Extinguisher\"\n",
      "has\"Hot Tub\"\n",
      "hasDryer\n",
      "has\"Air Conditioning\"\n",
      "has\"Laptop Friendly Workspace\"\n",
      "has\"Suitable for Events\"\n",
      "hasKitchen\n",
      "has\"Family/Kid Friendly\"\n",
      "has\"translation missing: en.hosting_amenity_49\"\n",
      "hasShampoo\n",
      "hasHeating\n",
      "has\"Hair Dryer\"\n",
      "hasCat(s)\n",
      "has\"Smoke Detector\"\n",
      "hasIron\n",
      "has\"Free Parking on Premises\"\n",
      "has\"Pets live on this property\"\n",
      "has\"Safety Card\"\n",
      "has\"Smoking Allowed\"\n",
      "has\"Pets Allowed\"\n",
      "has\"Wheelchair Accessible\"\n",
      "has\"First Aid Kit\"\n",
      "hasDog(s)\n",
      "has\"Wireless Internet\"\n",
      "has\"Cable TV\"\n",
      "hasPool\n",
      "hasBreakfast\n",
      "hasDoorman\n",
      "has\"Lock on Bedroom Door\"\n",
      "has\"24-Hour Check-in\"\n",
      "uses 'phone'\n",
      "uses 'linkedin'\n",
      "uses'email'\n",
      "uses 'manual_online'\n",
      "uses 'facebook'\n",
      "uses 'amex'\n",
      "uses'phone'\n",
      "uses 'sent_id'\n",
      "uses 'jumio'\n",
      "uses 'google'\n",
      "uses 'manual_offline'\n",
      "uses 'kba'\n",
      "uses 'reviews'\n",
      "distance_from_ocean\n",
      "host_response_time_enc\n",
      "calendar_updated_enc\n",
      "bed_type_enc\n",
      "jurisdiction_names_enc\n",
      "zipcode_enc\n",
      "host_is_superhost_bin\n",
      "is_location_exact_bin\n",
      "host_has_profile_pic_bin\n",
      "host_identity_verified_bin\n",
      "instant_bookable_bin\n",
      "require_guest_profile_picture_bin\n",
      "require_guest_phone_verification_bin\n"
     ]
    }
   ],
   "source": [
    "for i in new10.columns:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today = datetime.date.today()\n",
    "count+=1\n",
    "filename = 'listings_augmented_' + str(today) + '_V' + str(count) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listings_augmented_2018-05-18_V1.csv\n"
     ]
    }
   ],
   "source": [
    "print filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For the date, you can use datetime.date.today() or datetime.datetime.now().date().\\n\\nFor the time, you can use datetime.datetime.now().time().'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"For the date, you can use datetime.date.today() or datetime.datetime.now().date().\n",
    "\n",
    "For the time, you can use datetime.datetime.now().time().\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new11.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listings Shape at Each Iteration\n",
      "(6608, 95)\n",
      "(5753, 123)\n",
      "(5753, 291)\n",
      "(5753, 323)\n",
      "(5753, 331)\n",
      "(5753, 523)\n",
      "(5753, 531)\n",
      "(5753, 575)\n",
      "(5753, 588)\n",
      "(5753, 589)\n",
      "(5753, 594)\n",
      "(5753, 601)\n",
      "(5753, 601)\n"
     ]
    }
   ],
   "source": [
    "print \"Listings Shape at Each Iteration\"\n",
    "print listings_original.shape\n",
    "print listings.shape\n",
    "print new.shape\n",
    "print new2.shape\n",
    "print new3.shape\n",
    "print new4.shape\n",
    "print new5.shape\n",
    "print new6.shape\n",
    "print new7.shape\n",
    "print new8.shape\n",
    "print new9.shape\n",
    "print new10.shape\n",
    "print new11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
