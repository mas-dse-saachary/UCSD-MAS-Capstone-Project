{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation Notebook Consolidating All Data Prep code into linear process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from StringIO import StringIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings_original = pd.read_csv('Datasources/inside_airbnb/listings.csv')\n",
    "calendar_original = pd.read_csv('Datasources/inside_airbnb/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_columns(listings, cols):\n",
    "    chars = \"%$\"\n",
    "    for i in cols:\n",
    "        listings[i].astype(str).map(lambda x: x.rstrip(chars))\n",
    "        listings[i] = listings[i].apply(pd.to_numeric, errors='coerce')\n",
    "        listings[i].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "    return listings        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings = parse_columns(listings_original, ['host_response_rate', 'cleaning_fee',\\\n",
    "                                     'host_acceptance_rate','extra_people',\\\n",
    "                                     'weekly_price', 'monthly_price', 'security_deposit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to produce 4 listings dataframes (whole, holiday, wke, wkd) with listing mean price\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "def get_mean_price(cal, listings):\n",
    "    \n",
    "    cal['price'] = cal['price'].astype(str).map(lambda x: x.lstrip('$'))\n",
    "    cal['price'] = cal['price'].apply(pd.to_numeric, errors='coerce')\n",
    "    cal['price'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "    cal['date'] = pd.to_datetime(cal['date'])\n",
    "    cal['month'] = cal['date'].apply(lambda x: x.month)\n",
    "    cal['day'] = cal['date'].apply(lambda x: x.day)\n",
    "    cal['day_of_week'] = cal['date'].dt.weekday_name\n",
    "    \n",
    "    cl = calendar()\n",
    "    holidays = cl.holidays(start=cal['date'].min(), end=cal['date'].max())\n",
    "    \n",
    "    cal['holiday'] = cal['date'].isin(holidays)\n",
    "    cal = cal[(cal['date']>'2016-07-06')&(cal['date']<'2016-10-06')]\n",
    "    \n",
    "    c = cal.loc[cal.available!='f']\n",
    "    c = c[['listing_id','date','price','month','day_of_week','holiday']]\n",
    "    c=c.fillna(c.mean())\n",
    "    \n",
    "    c_hol = c[c['holiday']==True]\n",
    "    c_wke = c[(c['holiday']==False)&((c['day_of_week']=='Sunday')|(c['day_of_week']=='Saturday'))]\n",
    "    c_wkd = c[(~c.isin(c_hol['date']))&(~c.isin(c_wke['date']))]\n",
    "\n",
    "\n",
    "    price_hol_dict = {'price': c_hol.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c_hol.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c_hol.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c_hol.groupby(by='listing_id')['price'].fillna(0).std(),                 \n",
    "                  'skew_of_price': c_hol.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                     'median_price': c_hol.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "    price_wke_dict = {'price': c_wke.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c_wke.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c_wke.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c_wke.groupby(by='listing_id')['price'].fillna(0).std(),\n",
    "                  'skew_of_price': c_wke.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                     'median_price': c_wke.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "    price_wkd_dict = {'price': c_wkd.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c_wkd.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c_wkd.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c_wkd.groupby(by='listing_id')['price'].fillna(0).std(),\n",
    "                  'skew_of_price': c_wkd.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                     'median_price': c_wkd.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "    price_whole_dict = {'price': c.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c.groupby(by='listing_id')['price'].fillna(0).std(),\n",
    "                  'skew_of_price': c.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                       'median_price': c.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "\n",
    "    price_hol = pd.DataFrame(price_hol_dict)\n",
    "    price_wke = pd.DataFrame(price_wke_dict)\n",
    "    price_wkd = pd.DataFrame(price_wkd_dict)\n",
    "    price_c = pd.DataFrame(price_whole_dict)    \n",
    "    \n",
    "    price_hol = price_hol.reset_index()\n",
    "    price_wke = price_wke.reset_index()\n",
    "    price_wkd = price_wkd.reset_index()\n",
    "    price_c = price_c.reset_index()\n",
    "\n",
    "    listings_hol = listings.merge(price_hol, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_wke = listings.merge(price_wke, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_wkd = listings.merge(price_wkd, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_c = listings.merge(price_c, how='inner', left_on='id', right_on='listing_id')\n",
    "    \n",
    "    price_hol_new = price_hol.rename(columns = {'max_price': 'hol_max_price', 'min_price': 'hol_min_price', 'price': 'hol_price',\n",
    "                                           'skew_of_price': 'hol_skew_of_price', 'stdev_of_price': 'hol_stdev_of_price',\n",
    "                                               'median_price' : 'hol_median_price'})\n",
    "    price_wke_new = price_wke.rename(columns = {'max_price': 'wke_max_price', 'min_price': 'wke_min_price', 'price': 'wke_price',\n",
    "                                           'skew_of_price': 'wke_skew_of_price', 'stdev_of_price': 'wke_stdev_of_price',\n",
    "                                               'median_price' : 'wke_median_price'})\n",
    "    price_wkd_new = price_wkd.rename(columns = {'max_price': 'wkd_max_price', 'min_price': 'wkd_min_price', 'price': 'wkd_price',\n",
    "                                           'skew_of_price': 'wkd_skew_of_price', 'stdev_of_price': 'wkd_stdev_of_price',\n",
    "                                               'median_price' : 'wkd_median_price'})\n",
    "    \n",
    "    listings_c = listings_c.merge(price_hol_new, how='outer', left_on='id', right_on='listing_id')\n",
    "    listings_c = listings_c.merge(price_wke_new, how='outer', left_on='id', right_on='listing_id')\n",
    "    listings_c = listings_c.merge(price_wkd_new, how='outer', left_on='id', right_on='listing_id')\n",
    "    \n",
    "    L_hol = ['hol_max_price', 'hol_min_price', 'hol_price', 'hol_skew_of_price', 'hol_stdev_of_price', 'hol_median_price']\n",
    "    L_wke = ['wke_max_price', 'wke_min_price', 'wke_price', 'wke_skew_of_price', 'wke_stdev_of_price', 'wke_median_price']\n",
    "    L_wkd = ['wkd_max_price', 'wkd_min_price', 'wkd_price', 'wkd_skew_of_price', 'wkd_stdev_of_price', 'wkd_median_price']\n",
    "    \n",
    "    listings_c[L_hol + L_wke + L_wkd] = listings_c[L_hol + L_wke + L_wkd].fillna(0)\n",
    "    listings_c = listings_c.drop(['listing_id_y'], axis = 1)\n",
    "    listings_c['listing_id_x'] = listings_c['listing_id_x'].fillna(0)\n",
    "    \n",
    "    #len(cal['listing_id'].astype(str).unique())\n",
    "    #count = len(c['listing_id'].astype(str).unique())\n",
    "    \n",
    "    #print('Due to the above filtering on calendar, the right total count of listings is: ' %(count))\n",
    "    \n",
    "    return listings_hol, listings_wke, listings_wkd, listings_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings_hol, listings_wke, listings_wkd, listings = get_mean_price(calendar_original, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                     0\n",
       "listing_url                            0\n",
       "scrape_id                              0\n",
       "last_scraped                           0\n",
       "name                                   0\n",
       "summary                              125\n",
       "space                                860\n",
       "description                            1\n",
       "experiences_offered                    0\n",
       "neighborhood_overview               1420\n",
       "notes                               1945\n",
       "transit                             1552\n",
       "access                              1311\n",
       "interaction                         1490\n",
       "house_rules                          905\n",
       "thumbnail_url                        755\n",
       "medium_url                           755\n",
       "picture_url                            0\n",
       "xl_picture_url                       755\n",
       "host_id                                0\n",
       "host_url                               0\n",
       "host_name                              0\n",
       "host_since                             0\n",
       "host_location                         10\n",
       "host_about                          1191\n",
       "host_response_time                   215\n",
       "host_response_rate                  4000\n",
       "host_acceptance_rate                4000\n",
       "host_is_superhost                      0\n",
       "host_thumbnail_url                     0\n",
       "                                    ... \n",
       "availability_60                        0\n",
       "availability_90                        0\n",
       "availability_365                       0\n",
       "calendar_last_scraped                  0\n",
       "number_of_reviews                      0\n",
       "first_review                        1061\n",
       "last_review                         1066\n",
       "review_scores_rating                1125\n",
       "review_scores_accuracy              1132\n",
       "review_scores_cleanliness           1132\n",
       "review_scores_checkin               1133\n",
       "review_scores_communication         1131\n",
       "review_scores_location              1131\n",
       "review_scores_value                 1132\n",
       "requires_license                       0\n",
       "license                             4000\n",
       "jurisdiction_names                   178\n",
       "instant_bookable                       0\n",
       "cancellation_policy                    0\n",
       "require_guest_profile_picture          0\n",
       "require_guest_phone_verification       0\n",
       "calculated_host_listings_count         0\n",
       "reviews_per_month                   1061\n",
       "listing_id                             0\n",
       "max_price                              0\n",
       "median_price                           0\n",
       "min_price                              0\n",
       "price_y                                0\n",
       "skew_of_price                          0\n",
       "stdev_of_price                         0\n",
       "Length: 102, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_hol.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                     0\n",
       "listing_url                            0\n",
       "scrape_id                              0\n",
       "last_scraped                           0\n",
       "name                                   0\n",
       "summary                              157\n",
       "space                               1275\n",
       "description                            1\n",
       "experiences_offered                    0\n",
       "neighborhood_overview               2041\n",
       "notes                               2801\n",
       "transit                             2213\n",
       "access                              1884\n",
       "interaction                         2097\n",
       "house_rules                         1292\n",
       "thumbnail_url                       1057\n",
       "medium_url                          1057\n",
       "picture_url                            0\n",
       "xl_picture_url                      1057\n",
       "host_id                                0\n",
       "host_url                               0\n",
       "host_name                              0\n",
       "host_since                             0\n",
       "host_location                         18\n",
       "host_about                          1804\n",
       "host_response_time                   377\n",
       "host_response_rate                  5753\n",
       "host_acceptance_rate                5753\n",
       "host_is_superhost                      0\n",
       "host_thumbnail_url                     0\n",
       "                                    ... \n",
       "require_guest_profile_picture          0\n",
       "require_guest_phone_verification       0\n",
       "calculated_host_listings_count         0\n",
       "reviews_per_month                   1608\n",
       "listing_id_x                           0\n",
       "max_price                              0\n",
       "median_price                           0\n",
       "min_price                              0\n",
       "price_y                                0\n",
       "skew_of_price                          0\n",
       "stdev_of_price                         0\n",
       "hol_max_price                          0\n",
       "hol_median_price                       0\n",
       "hol_min_price                          0\n",
       "hol_price                              0\n",
       "hol_skew_of_price                      0\n",
       "hol_stdev_of_price                     0\n",
       "listing_id_x                           0\n",
       "wke_max_price                          0\n",
       "wke_median_price                       0\n",
       "wke_min_price                          0\n",
       "wke_price                              0\n",
       "wke_skew_of_price                      0\n",
       "wke_stdev_of_price                     0\n",
       "wkd_max_price                          0\n",
       "wkd_median_price                       0\n",
       "wkd_min_price                          0\n",
       "wkd_price                              0\n",
       "wkd_skew_of_price                      0\n",
       "wkd_stdev_of_price                     0\n",
       "Length: 121, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use n components in place of n topics when using gridsearchcv\n",
    "def create_topics(pdseries, listings):\n",
    "        corpus = pdseries.fillna('none')\n",
    "        \n",
    "        vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "        \n",
    "        data_vectorized = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        lda_model = LatentDirichletAllocation(n_topics=20,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "        \n",
    "        lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "        # column names\n",
    "        col_name = pd.DataFrame(pdseries).columns[0]\n",
    "        topicnames = [str(col_name) + \"-\" + \"Topic\" + str(i) for i in range(lda_model.n_topics)]\n",
    "\n",
    "        # index names\n",
    "        docnames = [str(col_name) + \"-\" + \"Doc\" + str(i) for i in range(len(corpus))]\n",
    "\n",
    "        # Make the pandas dataframe\n",
    "        df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "        # Get dominant topic for each document\n",
    "        dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "        df_document_topic[str(col_name) + \"-\" + 'Dominant_Topic'] = dominant_topic\n",
    "        \n",
    "        df_document_topic.index = [i for i in range(len(df_document_topic))]\n",
    "        \n",
    "        df_document_topic = df_document_topic.fillna(0)\n",
    "        \n",
    "        out = df_document_topic.merge(listings, left_index=True, right_index=True)\n",
    "        out = out.astype('str')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new = listings.copy()\n",
    "for i in text_features:\n",
    "    new = create_topics(listings[i], new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def create_txt_features(pdseries, listings):\n",
    "    \n",
    "    textLength = []\n",
    "    textWordsPerc = []\n",
    "    textPuncPerc = []\n",
    "    textDigitsPerc = []\n",
    "\n",
    "    for i in pdseries:\n",
    "        tokens = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        textLength.append(len(tokens))\n",
    "\n",
    "        if len(tokens)==0:\n",
    "            textWordsPerc.append(0)\n",
    "            textPuncPerc.append(0)\n",
    "            textDigitsPerc.append(0)\n",
    "\n",
    "        else:\n",
    "            textWordsPerc.append(len(i.split())/float(len(tokens)))\n",
    "            textPuncPerc.append(len(''.join(c for c in i if c in string.punctuation))/float(len(tokens)))\n",
    "            textDigitsPerc.append(len(''.join(c for c in i if c in string.digits))/float(len(tokens)))\n",
    "\n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    \n",
    "    textLength_varname = str(col_name) + '_TextLength'\n",
    "    textWordsPerc_varname = str(col_name) + '_TextWordsPerc'\n",
    "    textPuncPerc_varname = str(col_name) + '_TextPuncPerc'\n",
    "    textDigitsPerc_varname = str(col_name) + '_TextDigitsPerc'\n",
    "    \n",
    "    listings[textLength_varname] = textLength\n",
    "    listings[textWordsPerc_varname] = textWordsPerc\n",
    "    listings[textPuncPerc_varname] = textPuncPerc\n",
    "    listings[textDigitsPerc_varname] = textDigitsPerc\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new2 = new.copy()\n",
    "for i in text_features:\n",
    "    new2 = create_txt_features(new[i], new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(pdseries, listings):\n",
    "    \n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    varname = str(col_name) + \"_LexicalDiversity\"\n",
    "    \n",
    "    lx_div = pd.Series([len(i)/len(set(i)) for i in pdseries])\n",
    "    listings[varname] = lx_div\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new3 = new2.copy()\n",
    "for i in text_features:\n",
    "    new3 = lexical_diversity(new2[i], new3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_grammar(pdseries, listings):\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.tag import pos_tag, map_tag\n",
    "    from collections import Counter\n",
    "      \n",
    "    df = pd.DataFrame()\n",
    "    for text in pdseries:\n",
    "        \n",
    "        col_name = pd.DataFrame(pdseries).columns[0]\n",
    "        \n",
    "        \n",
    "        tokenized_text = nltk.word_tokenize(text.decode('utf-8'))\n",
    "        grammar = [i[1] for i in nltk.pos_tag(tokenized_text, tagset='universal')]\n",
    "        \n",
    "        counter = Counter(grammar)\n",
    "        fr = pd.DataFrame(counter, index=[0])\n",
    "        fr.columns = [str(col_name) + '_' + str(i) for i in fr.columns]\n",
    "        \n",
    "        fr2 = fr/len(tokenized_text)\n",
    "        fr2.columns = [str(i) + '_tokens_sum_ratio' for i in fr2.columns]\n",
    "        \n",
    "        fr3 = pd.concat([fr, fr2], ignore_index=True)\n",
    "        \n",
    "        df = pd.concat([df, fr3], ignore_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    df = df.fillna(0)\n",
    "        \n",
    "    return listings.merge(df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'taggers/universal_tagset/en-ptb.map' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\sanka/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\sanka\\\\Anaconda2\\\\nltk_data'\n    - 'C:\\\\Users\\\\sanka\\\\Anaconda2\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\sanka\\\\AppData\\\\Roaming\\\\nltk_data'\n    - u''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c00d561b8eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnew4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_grammar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-dc572505cf6c>\u001b[0m in \u001b[0;36mextract_grammar\u001b[0;34m(pdseries, listings)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mgrammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'universal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\nltk\\tag\\__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \"\"\"\n\u001b[1;32m    110\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\nltk\\tag\\__init__.pyc\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en-ptb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\nltk\\tag\\mapping.pyc\u001b[0m in \u001b[0;36mmap_tag\u001b[0;34m(source, target, source_tag)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'en-brown'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagset_mapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource_tag\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\nltk\\tag\\mapping.pyc\u001b[0m in \u001b[0;36mtagset_mapping\u001b[0;34m(source, target)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_MAPPINGS\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_MAPPINGS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'universal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0m_load_universal_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_MAPPINGS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\nltk\\tag\\mapping.pyc\u001b[0m in \u001b[0;36m_load_universal_map\u001b[0;34m(fileid)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_load_universal_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_UNIVERSAL_DATA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.map'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[1;31m# When mapping to the Universal Tagset,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\nltk\\data.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\nltk\\data.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\sanka\\Anaconda2\\lib\\site-packages\\nltk\\data.pyc\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'taggers/universal_tagset/en-ptb.map' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\sanka/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\sanka\\\\Anaconda2\\\\nltk_data'\n    - 'C:\\\\Users\\\\sanka\\\\Anaconda2\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\sanka\\\\AppData\\\\Roaming\\\\nltk_data'\n    - u''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new4 = new3.copy()\n",
    "for i in text_features:\n",
    "    new4 = extract_grammar(new3[i], new4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_Clusterer(pdseries, listings):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(pdseries)\n",
    "    true_k = 10\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "    model.fit(X)\n",
    "    \n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    varname = str(col_name) + \"_KmeansCluster\"\n",
    "    \n",
    "    listings[varname] = pd.Series(model.labels_)\n",
    "    listings[varname] = listings[varname].fillna(0)\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new5 = new4.copy()\n",
    "for i in text_features:\n",
    "    new5 = kmeans_Clusterer(new4[i], new5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_to_set(x):\n",
    "    c = set()\n",
    "    for w in x[1:-1].split(\",\"):\n",
    "        c.add(w)\n",
    "        \n",
    "    return c\n",
    "\n",
    "def has_amenity(x, amen_):\n",
    "    if amen_ in x:\n",
    "        return 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_amenities(listings):\n",
    "    listings['amenities_set'] = listings['amenities'].fillna('{}').map(string_to_set)\n",
    "    all_amenities = set()\n",
    "    \n",
    "    for idx in listings['amenities'].fillna('{}').map(string_to_set).index:\n",
    "        all_amenities = all_amenities.union(listings['amenities'].fillna('{}').map(string_to_set)[idx])\n",
    "    \n",
    "    for amen in all_amenities:\n",
    "        \n",
    "        if len(amen.split(' ')) == 1:\n",
    "            listings['has_' + amen] = 0\n",
    "            listings['has_' + amen] = listings['amenities_set'].map(lambda x: has_amenity(x, amen))\n",
    "            continue\n",
    "            \n",
    "        if \"\" in amen:\n",
    "            amen = amen[1:-1].replace(' ', '_')\n",
    "            \n",
    "        listings['has_' + amen] = 0\n",
    "        listings['has_' + amen] = listings['amenities_set'].map(lambda x: has_amenity(x, amen))\n",
    "        \n",
    "    \n",
    "    has_amenties_list = []\n",
    "    for amen in all_amenities:\n",
    "        \n",
    "        if len(amen.split(' ')) == 1:\n",
    "            has_amenties_list.append('has_' + amen)\n",
    "            continue\n",
    "            \n",
    "        if \"\" in amen:\n",
    "            amen = amen[1:-1].replace(' ', '_')\n",
    "            \n",
    "        has_amenties_list.append('has_' + amen)\n",
    "        \n",
    "    listings[has_amenties_list] = listings[has_amenties_list].fillna(0)\n",
    "    \n",
    "    return listings      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new6 = new5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new6 = add_amenities(new6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_host_verifications(listings):\n",
    "    a = listings['host_verifications'].map(lambda x: x[1:-1]).map(lambda j: j.split(',')).map(lambda k: set(k))\n",
    "    all_host_verifications = set()\n",
    "    \n",
    "    for w in a.index:\n",
    "        all_host_verifications = all_host_verifications.union(a[w])\n",
    "        \n",
    "    for w in all_host_verifications:\n",
    "        \n",
    "        if '' in w:\n",
    "            w = w.strip()[1:-1].replace(' ', '_')\n",
    "            \n",
    "        listings['uses_' + w] = 0\n",
    "        listings['uses_' + w] = a.map(lambda x: has_amenity(x, w))\n",
    "        \n",
    "    \n",
    "    uses_verification_list = []\n",
    "    for veri in all_host_verifications:\n",
    "        \n",
    "        if '' in veri:\n",
    "            veri = veri.strip()[1:-1].replace(' ', '_')\n",
    "            \n",
    "        uses_verification_list.append('uses_' + veri)\n",
    "        \n",
    "    listings[uses_verification_list] = listings[uses_verification_list].fillna(0)\n",
    "    \n",
    "    return listings      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new7 = new6.copy()\n",
    "new7 = add_host_verifications(new7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_distance_from_ocean(listings):\n",
    "    listings['distance_from_ocean'] = 0\n",
    "    listings['distance_from_ocean'] = listings['distance_from_ocean'].astype('float')\n",
    "    \n",
    "    for w in listings.index:\n",
    "        p = float(listings['latitude'][w])\n",
    "        q = float(listings['longitude'][w])\n",
    "        lon_diff = (q + 117.235585)*np.pi/180\n",
    "        lat_diff = (p - 32.802458)*np.pi/180\n",
    "        a = np.sin(lat_diff/2)**2 + np.cos(p*np.pi/180)*np.cos(32.802458*np.pi/180)*(np.sin(lon_diff/2)**2)\n",
    "        c = np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        d = 6371.00*float(c)\n",
    "        listings['distance_from_ocean'][w] = d\n",
    "        \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new8 = new7.copy()\n",
    "new8 = add_distance_from_ocean(new8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(listings, encoded_features):\n",
    "    \n",
    "    label_enc = LabelEncoder()\n",
    "    \n",
    "    for col in encoded_features:\n",
    "        \n",
    "        listings[col] = listings[col].astype(str)\n",
    "        \n",
    "        var_name = str(col) + '_enc'\n",
    "        listings[var_name] = label_enc.fit_transform(listings[col])\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_vars = ['host_response_time', 'calendar_updated', 'bed_type', 'jurisdiction_names', 'zipcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new9 = new8.copy()\n",
    "new9 = encoder(new9, encoded_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Caution!!! The input features are not dropped by the following to columns - they must be dropped as part of modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarizer(listings, binarized_features):\n",
    "    \n",
    "    label_enc = LabelBinarizer()\n",
    "    \n",
    "    for col in binarized_features:\n",
    "        \n",
    "        listings[col] = listings[col].astype(str)\n",
    "        \n",
    "        var_name = str(col) + '_bin'\n",
    "        listings[var_name] = label_enc.fit_transform(listings[col])\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binarized_vars = ['host_is_superhost','is_location_exact','host_has_profile_pic','host_identity_verified',\n",
    "                  'instant_bookable','require_guest_profile_picture','require_guest_phone_verification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new10 = new9.copy()\n",
    "new10 = binarizer(new10, binarized_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes list of features that should be numeric and transforms them to float\n",
    "#Also takes care of the topic features - these need not be input into the features parameter\n",
    "def make_numeric(listings, features):\n",
    "    #Taking Care of topics features\n",
    "    topic_cols = listings.filter(regex='Topic').columns\n",
    "    listings[topic_cols] = listings[topic_cols].astype(float)\n",
    "    \n",
    "    #Now transforming the other features:\n",
    "    for i in features:\n",
    "        listings[i] = listings[i].astype(float)\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings.wke_max_price.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_columns_new(listings, cols):\n",
    "    \n",
    "    topic_cols = listings.filter(regex='Topic').columns\n",
    "    listings[topic_cols] = listings[topic_cols].astype(float)\n",
    "    \n",
    "    for i in cols:\n",
    "        listings[i] = listings[i].replace('$', '')\n",
    "        #listings[i] = listings[i].replace('%', '')\n",
    "        listings[i] = listings[i].apply(pd.to_numeric, errors='coerce')\n",
    "        listings[i].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house_rules-Topic0 object\n",
      "house_rules-Topic1 object\n",
      "house_rules-Topic2 object\n",
      "house_rules-Topic3 object\n",
      "house_rules-Topic4 object\n",
      "house_rules-Topic5 object\n",
      "house_rules-Topic6 object\n",
      "house_rules-Topic7 object\n",
      "house_rules-Topic8 object\n",
      "house_rules-Topic9 object\n",
      "house_rules-Topic10 object\n",
      "house_rules-Topic11 object\n",
      "house_rules-Topic12 object\n",
      "house_rules-Topic13 object\n",
      "house_rules-Topic14 object\n",
      "house_rules-Topic15 object\n",
      "house_rules-Topic16 object\n",
      "house_rules-Topic17 object\n",
      "house_rules-Topic18 object\n",
      "house_rules-Topic19 object\n",
      "house_rules-Dominant_Topic object\n",
      "interaction-Topic0 object\n",
      "interaction-Topic1 object\n",
      "interaction-Topic2 object\n",
      "interaction-Topic3 object\n",
      "interaction-Topic4 object\n",
      "interaction-Topic5 object\n",
      "interaction-Topic6 object\n",
      "interaction-Topic7 object\n",
      "interaction-Topic8 object\n",
      "interaction-Topic9 object\n",
      "interaction-Topic10 object\n",
      "interaction-Topic11 object\n",
      "interaction-Topic12 object\n",
      "interaction-Topic13 object\n",
      "interaction-Topic14 object\n",
      "interaction-Topic15 object\n",
      "interaction-Topic16 object\n",
      "interaction-Topic17 object\n",
      "interaction-Topic18 object\n",
      "interaction-Topic19 object\n",
      "interaction-Dominant_Topic object\n",
      "access-Topic0 object\n",
      "access-Topic1 object\n",
      "access-Topic2 object\n",
      "access-Topic3 object\n",
      "access-Topic4 object\n",
      "access-Topic5 object\n",
      "access-Topic6 object\n",
      "access-Topic7 object\n",
      "access-Topic8 object\n",
      "access-Topic9 object\n",
      "access-Topic10 object\n",
      "access-Topic11 object\n",
      "access-Topic12 object\n",
      "access-Topic13 object\n",
      "access-Topic14 object\n",
      "access-Topic15 object\n",
      "access-Topic16 object\n",
      "access-Topic17 object\n",
      "access-Topic18 object\n",
      "access-Topic19 object\n",
      "access-Dominant_Topic object\n",
      "transit-Topic0 object\n",
      "transit-Topic1 object\n",
      "transit-Topic2 object\n",
      "transit-Topic3 object\n",
      "transit-Topic4 object\n",
      "transit-Topic5 object\n",
      "transit-Topic6 object\n",
      "transit-Topic7 object\n",
      "transit-Topic8 object\n",
      "transit-Topic9 object\n",
      "transit-Topic10 object\n",
      "transit-Topic11 object\n",
      "transit-Topic12 object\n",
      "transit-Topic13 object\n",
      "transit-Topic14 object\n",
      "transit-Topic15 object\n",
      "transit-Topic16 object\n",
      "transit-Topic17 object\n",
      "transit-Topic18 object\n",
      "transit-Topic19 object\n",
      "transit-Dominant_Topic object\n",
      "notes-Topic0 object\n",
      "notes-Topic1 object\n",
      "notes-Topic2 object\n",
      "notes-Topic3 object\n",
      "notes-Topic4 object\n",
      "notes-Topic5 object\n",
      "notes-Topic6 object\n",
      "notes-Topic7 object\n",
      "notes-Topic8 object\n",
      "notes-Topic9 object\n",
      "notes-Topic10 object\n",
      "notes-Topic11 object\n",
      "notes-Topic12 object\n",
      "notes-Topic13 object\n",
      "notes-Topic14 object\n",
      "notes-Topic15 object\n",
      "notes-Topic16 object\n",
      "notes-Topic17 object\n",
      "notes-Topic18 object\n",
      "notes-Topic19 object\n",
      "notes-Dominant_Topic object\n",
      "neighborhood_overview-Topic0 object\n",
      "neighborhood_overview-Topic1 object\n",
      "neighborhood_overview-Topic2 object\n",
      "neighborhood_overview-Topic3 object\n",
      "neighborhood_overview-Topic4 object\n",
      "neighborhood_overview-Topic5 object\n",
      "neighborhood_overview-Topic6 object\n",
      "neighborhood_overview-Topic7 object\n",
      "neighborhood_overview-Topic8 object\n",
      "neighborhood_overview-Topic9 object\n",
      "neighborhood_overview-Topic10 object\n",
      "neighborhood_overview-Topic11 object\n",
      "neighborhood_overview-Topic12 object\n",
      "neighborhood_overview-Topic13 object\n",
      "neighborhood_overview-Topic14 object\n",
      "neighborhood_overview-Topic15 object\n",
      "neighborhood_overview-Topic16 object\n",
      "neighborhood_overview-Topic17 object\n",
      "neighborhood_overview-Topic18 object\n",
      "neighborhood_overview-Topic19 object\n",
      "neighborhood_overview-Dominant_Topic object\n",
      "description-Topic0 object\n",
      "description-Topic1 object\n",
      "description-Topic2 object\n",
      "description-Topic3 object\n",
      "description-Topic4 object\n",
      "description-Topic5 object\n",
      "description-Topic6 object\n",
      "description-Topic7 object\n",
      "description-Topic8 object\n",
      "description-Topic9 object\n",
      "description-Topic10 object\n",
      "description-Topic11 object\n",
      "description-Topic12 object\n",
      "description-Topic13 object\n",
      "description-Topic14 object\n",
      "description-Topic15 object\n",
      "description-Topic16 object\n",
      "description-Topic17 object\n",
      "description-Topic18 object\n",
      "description-Topic19 object\n",
      "description-Dominant_Topic object\n",
      "space-Topic0 object\n",
      "space-Topic1 object\n",
      "space-Topic2 object\n",
      "space-Topic3 object\n",
      "space-Topic4 object\n",
      "space-Topic5 object\n",
      "space-Topic6 object\n",
      "space-Topic7 object\n",
      "space-Topic8 object\n",
      "space-Topic9 object\n",
      "space-Topic10 object\n",
      "space-Topic11 object\n",
      "space-Topic12 object\n",
      "space-Topic13 object\n",
      "space-Topic14 object\n",
      "space-Topic15 object\n",
      "space-Topic16 object\n",
      "space-Topic17 object\n",
      "space-Topic18 object\n",
      "space-Topic19 object\n",
      "space-Dominant_Topic object\n",
      "id object\n",
      "listing_url object\n",
      "scrape_id object\n",
      "last_scraped object\n",
      "name object\n",
      "summary object\n",
      "space object\n",
      "description object\n",
      "experiences_offered object\n",
      "neighborhood_overview object\n",
      "notes object\n",
      "transit object\n",
      "access object\n",
      "interaction object\n",
      "house_rules object\n",
      "thumbnail_url object\n",
      "medium_url object\n",
      "picture_url object\n",
      "xl_picture_url object\n",
      "host_id object\n",
      "host_url object\n",
      "host_name object\n",
      "host_since object\n",
      "host_location object\n",
      "host_about object\n",
      "host_response_time object\n",
      "host_response_rate object\n",
      "host_acceptance_rate object\n",
      "host_is_superhost object\n",
      "host_thumbnail_url object\n",
      "host_picture_url object\n",
      "host_neighbourhood object\n",
      "host_listings_count object\n",
      "host_total_listings_count object\n",
      "host_verifications object\n",
      "host_has_profile_pic object\n",
      "host_identity_verified object\n",
      "street object\n",
      "neighbourhood object\n",
      "neighbourhood_cleansed object\n",
      "neighbourhood_group_cleansed object\n",
      "city object\n",
      "state object\n",
      "zipcode object\n",
      "market object\n",
      "smart_location object\n",
      "country_code object\n",
      "country object\n",
      "latitude object\n",
      "longitude object\n",
      "is_location_exact object\n",
      "property_type object\n",
      "room_type object\n",
      "accommodates object\n",
      "bathrooms object\n",
      "bedrooms object\n",
      "beds object\n",
      "bed_type object\n",
      "amenities object\n",
      "square_feet object\n",
      "price_x object\n",
      "weekly_price object\n",
      "monthly_price object\n",
      "security_deposit object\n",
      "cleaning_fee object\n",
      "guests_included object\n",
      "extra_people object\n",
      "minimum_nights object\n",
      "maximum_nights object\n",
      "calendar_updated object\n",
      "has_availability object\n",
      "availability_30 object\n",
      "availability_60 object\n",
      "availability_90 object\n",
      "availability_365 object\n",
      "calendar_last_scraped object\n",
      "number_of_reviews object\n",
      "first_review object\n",
      "last_review object\n",
      "review_scores_rating object\n",
      "review_scores_accuracy object\n",
      "review_scores_cleanliness object\n",
      "review_scores_checkin object\n",
      "review_scores_communication object\n",
      "review_scores_location object\n",
      "review_scores_value object\n",
      "requires_license object\n",
      "license object\n",
      "jurisdiction_names object\n",
      "instant_bookable object\n",
      "cancellation_policy object\n",
      "require_guest_profile_picture object\n",
      "require_guest_phone_verification object\n",
      "calculated_host_listings_count object\n",
      "reviews_per_month object\n",
      "listing_id_x object\n",
      "max_price object\n",
      "median_price object\n",
      "min_price object\n",
      "price_y object\n",
      "skew_of_price object\n",
      "stdev_of_price object\n",
      "hol_max_price object\n",
      "hol_median_price object\n",
      "hol_min_price object\n",
      "hol_price object\n",
      "hol_skew_of_price object\n",
      "hol_stdev_of_price object\n",
      "listing_id_x object\n",
      "wke_max_price object\n",
      "wke_median_price object\n",
      "wke_min_price object\n",
      "wke_price object\n",
      "wke_skew_of_price object\n",
      "wke_stdev_of_price object\n",
      "wkd_max_price object\n",
      "wkd_median_price object\n",
      "wkd_min_price object\n",
      "wkd_price object\n",
      "wkd_skew_of_price object\n",
      "wkd_stdev_of_price object\n",
      "space_TextLength int64\n",
      "space_TextWordsPerc float64\n",
      "space_TextPuncPerc float64\n",
      "space_TextDigitsPerc float64\n",
      "description_TextLength int64\n",
      "description_TextWordsPerc float64\n",
      "description_TextPuncPerc float64\n",
      "description_TextDigitsPerc float64\n",
      "neighborhood_overview_TextLength int64\n",
      "neighborhood_overview_TextWordsPerc float64\n",
      "neighborhood_overview_TextPuncPerc float64\n",
      "neighborhood_overview_TextDigitsPerc float64\n",
      "notes_TextLength int64\n",
      "notes_TextWordsPerc float64\n",
      "notes_TextPuncPerc float64\n",
      "notes_TextDigitsPerc float64\n",
      "transit_TextLength int64\n",
      "transit_TextWordsPerc float64\n",
      "transit_TextPuncPerc float64\n",
      "transit_TextDigitsPerc float64\n",
      "access_TextLength int64\n",
      "access_TextWordsPerc float64\n",
      "access_TextPuncPerc float64\n",
      "access_TextDigitsPerc float64\n",
      "interaction_TextLength int64\n",
      "interaction_TextWordsPerc float64\n",
      "interaction_TextPuncPerc float64\n",
      "interaction_TextDigitsPerc float64\n",
      "house_rules_TextLength int64\n",
      "house_rules_TextWordsPerc float64\n",
      "house_rules_TextPuncPerc float64\n",
      "house_rules_TextDigitsPerc float64\n",
      "space_LexicalDiversity int64\n",
      "description_LexicalDiversity int64\n",
      "neighborhood_overview_LexicalDiversity int64\n",
      "notes_LexicalDiversity int64\n",
      "transit_LexicalDiversity int64\n",
      "access_LexicalDiversity int64\n",
      "interaction_LexicalDiversity int64\n",
      "house_rules_LexicalDiversity int64\n",
      "space_KmeansCluster int32\n",
      "description_KmeansCluster int32\n",
      "neighborhood_overview_KmeansCluster int32\n",
      "notes_KmeansCluster int32\n",
      "transit_KmeansCluster int32\n",
      "access_KmeansCluster int32\n",
      "interaction_KmeansCluster int32\n",
      "house_rules_KmeansCluster int32\n",
      "amenities_set object\n",
      "has_ float64\n",
      "has_Other_pet(s) int64\n",
      "has_ssential int64\n",
      "has_Carbon_Monoxide_Detector int64\n",
      "has_Elevator_in_Building int64\n",
      "has_Indoor_Fireplace int64\n",
      "has_translation_missing:_en.hosting_amenity_50 int64\n",
      "has_nterne int64\n",
      "has_ashe int64\n",
      "has_anger int64\n",
      "has_Buzzer/Wireless_Intercom int64\n",
      "has_y int64\n",
      "has_Fire_Extinguisher int64\n",
      "has_Hot_Tub int64\n",
      "has_rye int64\n",
      "has_Air_Conditioning int64\n",
      "has_Laptop_Friendly_Workspace int64\n",
      "has_Suitable_for_Events int64\n",
      "has_itche int64\n",
      "has_Family/Kid_Friendly int64\n",
      "has_translation_missing:_en.hosting_amenity_49 int64\n",
      "has_hampo int64\n",
      "has_eatin int64\n",
      "has_Hair_Dryer int64\n",
      "has_at(s int64\n",
      "has_Smoke_Detector int64\n",
      "has_ro int64\n",
      "has_Free_Parking_on_Premises int64\n",
      "has_Pets_live_on_this_property int64\n",
      "has_Safety_Card int64\n",
      "has_Smoking_Allowed int64\n",
      "has_Pets_Allowed int64\n",
      "has_Wheelchair_Accessible int64\n",
      "has_First_Aid_Kit int64\n",
      "has_og(s int64\n",
      "has_Wireless_Internet int64\n",
      "has_Cable_TV int64\n",
      "has_oo int64\n",
      "has_reakfas int64\n",
      "has_oorma int64\n",
      "has_Lock_on_Bedroom_Door int64\n",
      "has_24-Hour_Check-in int64\n",
      "uses_phone int64\n",
      "uses_linkedin int64\n",
      "uses_email int64\n",
      "uses_manual_online int64\n",
      "uses_facebook int64\n",
      "uses_amex int64\n",
      "uses_sent_id int64\n",
      "uses_jumio int64\n",
      "uses_google int64\n",
      "uses_manual_offline int64\n",
      "uses_kba int64\n",
      "uses_reviews int64\n",
      "distance_from_ocean float64\n",
      "host_response_time_enc int64\n",
      "calendar_updated_enc int64\n",
      "bed_type_enc int64\n",
      "jurisdiction_names_enc int64\n",
      "zipcode_enc int64\n",
      "host_is_superhost_bin int32\n",
      "is_location_exact_bin int32\n",
      "host_has_profile_pic_bin int32\n",
      "host_identity_verified_bin int32\n",
      "instant_bookable_bin int32\n",
      "require_guest_profile_picture_bin int32\n",
      "require_guest_phone_verification_bin int32\n"
     ]
    }
   ],
   "source": [
    "for i,k in zip(new10.columns, new10.dtypes):\n",
    "    print i,k\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All nulls:\\n\\nsquare_feet                     5652\\nweekly_price                    5752\\nmonthly_price                   5752\\nsecurity_deposit                5752\\ncleaning_fee                    5752\\nextra_people                    5752\\nhas_availability \\nlicense\\nneighbourhood_group_cleansed\\n\\nhost_response_rate              5752\\nhost_acceptance_rate            5752\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = ['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', \n",
    "               'beds', 'guests_included', 'minimum_nights',\n",
    "               'maximum_nights', 'availability_30', 'availability_60','availability_90',\n",
    "               'availability_365', 'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy',\n",
    "               'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n",
    "               'review_scores_location', 'review_scores_value', 'calculated_host_listings_count', \n",
    "               'reviews_per_month', 'max_price','median_price','min_price','price_y','skew_of_price',\n",
    "                'stdev_of_price','hol_max_price','hol_median_price','hol_min_price','hol_price',\n",
    "                'hol_skew_of_price','hol_stdev_of_price','listing_id_x','wke_max_price','wke_median_price',\n",
    "                'wke_min_price','wke_price','wke_skew_of_price','wke_stdev_of_price','wkd_max_price',\n",
    "                'wkd_median_price','wkd_min_price','wkd_price','wkd_skew_of_price','wkd_stdev_of_price']\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"All nulls:\n",
    "\n",
    "square_feet                     5652\n",
    "weekly_price                    5752\n",
    "monthly_price                   5752\n",
    "security_deposit                5752\n",
    "cleaning_fee                    5752\n",
    "extra_people                    5752\n",
    "has_availability \n",
    "license\n",
    "neighbourhood_group_cleansed\n",
    "\n",
    "host_response_rate              5752\n",
    "host_acceptance_rate            5752\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new11 = new10.copy()\n",
    "new11 = parse_columns_new(new11, num_features)\n",
    "#new11 = make_numeric(new11, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "today = datetime.date.today()\n",
    "count+=1\n",
    "filename = 'listings_augmented_' + str(today) + '_V' + str(count) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listings_augmented_2018-05-18_V1.csv\n"
     ]
    }
   ],
   "source": [
    "print filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = filename.replace('1', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listings_augmented_2038-05-38_V3.csv\n"
     ]
    }
   ],
   "source": [
    "print filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For the date, you can use datetime.date.today() or datetime.datetime.now().date().\\n\\nFor the time, you can use datetime.datetime.now().time().'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"For the date, you can use datetime.date.today() or datetime.datetime.now().date().\n",
    "\n",
    "For the time, you can use datetime.datetime.now().time().\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new11.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listings Shape at Each Iteration\n",
      "(6608, 95)\n",
      "(5753, 121)\n",
      "(5753, 289)\n",
      "(5753, 321)\n",
      "(5753, 329)\n",
      "(5753, 329)\n",
      "(5753, 337)\n",
      "(5753, 380)\n",
      "(5753, 392)\n",
      "(5753, 393)\n",
      "(5753, 398)\n",
      "(5753, 405)\n",
      "(5753, 405)\n"
     ]
    }
   ],
   "source": [
    "print \"Listings Shape at Each Iteration\"\n",
    "print listings_original.shape\n",
    "print listings.shape\n",
    "print new.shape\n",
    "print new2.shape\n",
    "print new3.shape\n",
    "print new4.shape\n",
    "print new5.shape\n",
    "print new6.shape\n",
    "print new7.shape\n",
    "print new8.shape\n",
    "print new9.shape\n",
    "print new10.shape\n",
    "print new11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
