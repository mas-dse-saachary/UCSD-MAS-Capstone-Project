{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation Notebook Consolidating All Data Prep code into linear process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from StringIO import StringIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings_original = pd.read_csv('Datasources/inside_airbnb/listings.csv')\n",
    "calendar_original = pd.read_csv('Datasources/inside_airbnb/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_columns(listings, cols):\n",
    "    chars = \"%$\"\n",
    "    for i in cols:\n",
    "        listings[i].astype(str).map(lambda x: x.rstrip(chars))\n",
    "        listings[i] = listings[i].apply(pd.to_numeric, errors='coerce')\n",
    "        listings[i].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "    return listings        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings = parse_columns(listings_original, ['host_response_rate', 'cleaning_fee',\\\n",
    "                                     'host_acceptance_rate','extra_people',\\\n",
    "                                     'weekly_price', 'monthly_price', 'security_deposit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to produce 4 listings dataframes (whole, holiday, wke, wkd) with listing mean price\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "def get_mean_price(cal, listings):\n",
    "    \n",
    "    cal['price'] = cal['price'].astype(str).map(lambda x: x.lstrip('$'))\n",
    "    cal['price'] = cal['price'].apply(pd.to_numeric, errors='coerce')\n",
    "    cal['price'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "    cal['date'] = pd.to_datetime(cal['date'])\n",
    "    cal['month'] = cal['date'].apply(lambda x: x.month)\n",
    "    cal['day'] = cal['date'].apply(lambda x: x.day)\n",
    "    cal['day_of_week'] = cal['date'].dt.weekday_name\n",
    "    \n",
    "    cl = calendar()\n",
    "    holidays = cl.holidays(start=cal['date'].min(), end=cal['date'].max())\n",
    "    \n",
    "    cal['holiday'] = cal['date'].isin(holidays)\n",
    "    cal = cal[(cal['date']>'2016-07-06')&(cal['date']<'2016-10-06')]\n",
    "    \n",
    "    c = cal.loc[cal.available!='f']\n",
    "    c = c[['listing_id','date','price','month','day_of_week','holiday']]\n",
    "    c=c.fillna(c.mean())\n",
    "    \n",
    "    c_hol = c[c['holiday']==True]\n",
    "    c_wke = c[(c['holiday']==False)&((c['day_of_week']=='Sunday')|(c['day_of_week']=='Saturday'))]\n",
    "    c_wkd = c[(~c.isin(c_hol['date']))&(~c.isin(c_wke['date']))]\n",
    "    \n",
    "    mean_price_hol = pd.DataFrame(c_hol.groupby(by='listing_id')['price'].mean())\n",
    "    mean_price_wke = pd.DataFrame(c_wke.groupby(by='listing_id')['price'].mean())\n",
    "    mean_price_wkd = pd.DataFrame(c_wkd.groupby(by='listing_id')['price'].mean())\n",
    "    mean_price_c = pd.DataFrame(c.groupby(by='listing_id')['price'].mean())    \n",
    "    \n",
    "    mean_price_hol = mean_price_hol.reset_index()\n",
    "    mean_price_wke = mean_price_wke.reset_index()\n",
    "    mean_price_wkd = mean_price_wkd.reset_index()\n",
    "    mean_price_c = mean_price_c.reset_index()\n",
    "\n",
    "    listings_hol = listings.merge(mean_price_hol, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_wke = listings.merge(mean_price_wke, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_wkd = listings.merge(mean_price_wkd, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_c = listings.merge(mean_price_c, how='inner', left_on='id', right_on='listing_id')\n",
    "    \n",
    "    return listings_hol, listings_wke, listings_wkd, listings_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings_hol, listings_wke, listings_wkd, listings = get_mean_price(calendar_original, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use n components in place of n topics when using gridsearchcv\n",
    "def create_topics(pdseries, listings):\n",
    "        corpus = pdseries.fillna('none')\n",
    "        \n",
    "        vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "        \n",
    "        data_vectorized = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        lda_model = LatentDirichletAllocation(n_topics=20,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "        \n",
    "        lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "        # column names\n",
    "        col_name = pd.DataFrame(pdseries).columns[0]\n",
    "        topicnames = [str(col_name) + \"-\" + \"Topic\" + str(i) for i in range(lda_model.n_topics)]\n",
    "\n",
    "        # index names\n",
    "        docnames = [str(col_name) + \"-\" + \"Doc\" + str(i) for i in range(len(corpus))]\n",
    "\n",
    "        # Make the pandas dataframe\n",
    "        df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "        # Get dominant topic for each document\n",
    "        dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "        df_document_topic[str(col_name) + \"-\" + 'Dominant_Topic'] = dominant_topic\n",
    "        \n",
    "        df_document_topic.index = [i for i in range(len(df_document_topic))]\n",
    "        \n",
    "        out = df_document_topic.merge(listings, left_index=True, right_index=True)\n",
    "        out = out.astype('str')\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new = listings.copy()\n",
    "for i in text_features:\n",
    "    new = create_topics(listings[i], new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "house_rules-Topic0                  object\n",
       "house_rules-Topic1                  object\n",
       "house_rules-Topic2                  object\n",
       "house_rules-Topic3                  object\n",
       "house_rules-Topic4                  object\n",
       "house_rules-Topic5                  object\n",
       "house_rules-Topic6                  object\n",
       "house_rules-Topic7                  object\n",
       "house_rules-Topic8                  object\n",
       "house_rules-Topic9                  object\n",
       "house_rules-Topic10                 object\n",
       "house_rules-Topic11                 object\n",
       "house_rules-Topic12                 object\n",
       "house_rules-Topic13                 object\n",
       "house_rules-Topic14                 object\n",
       "house_rules-Topic15                 object\n",
       "house_rules-Topic16                 object\n",
       "house_rules-Topic17                 object\n",
       "house_rules-Topic18                 object\n",
       "house_rules-Topic19                 object\n",
       "house_rules-Dominant_Topic          object\n",
       "interaction-Topic0                  object\n",
       "interaction-Topic1                  object\n",
       "interaction-Topic2                  object\n",
       "interaction-Topic3                  object\n",
       "interaction-Topic4                  object\n",
       "interaction-Topic5                  object\n",
       "interaction-Topic6                  object\n",
       "interaction-Topic7                  object\n",
       "interaction-Topic8                  object\n",
       "                                     ...  \n",
       "minimum_nights                      object\n",
       "maximum_nights                      object\n",
       "calendar_updated                    object\n",
       "has_availability                    object\n",
       "availability_30                     object\n",
       "availability_60                     object\n",
       "availability_90                     object\n",
       "availability_365                    object\n",
       "calendar_last_scraped               object\n",
       "number_of_reviews                   object\n",
       "first_review                        object\n",
       "last_review                         object\n",
       "review_scores_rating                object\n",
       "review_scores_accuracy              object\n",
       "review_scores_cleanliness           object\n",
       "review_scores_checkin               object\n",
       "review_scores_communication         object\n",
       "review_scores_location              object\n",
       "review_scores_value                 object\n",
       "requires_license                    object\n",
       "license                             object\n",
       "jurisdiction_names                  object\n",
       "instant_bookable                    object\n",
       "cancellation_policy                 object\n",
       "require_guest_profile_picture       object\n",
       "require_guest_phone_verification    object\n",
       "calculated_host_listings_count      object\n",
       "reviews_per_month                   object\n",
       "listing_id                          object\n",
       "price_y                             object\n",
       "Length: 265, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def create_txt_features(pdseries, listings):\n",
    "    \n",
    "    textLength = []\n",
    "    textWordsPerc = []\n",
    "    textPuncPerc = []\n",
    "    textDigitsPerc = []\n",
    "\n",
    "    for i in pdseries:\n",
    "        tokens = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        textLength.append(len(tokens))\n",
    "\n",
    "        if len(tokens)==0:\n",
    "            textWordsPerc.append(0)\n",
    "            textPuncPerc.append(0)\n",
    "            textDigitsPerc.append(0)\n",
    "\n",
    "        else:\n",
    "            textWordsPerc.append(len(i.split())/float(len(tokens)))\n",
    "            textPuncPerc.append(len(''.join(c for c in i if c in string.punctuation))/float(len(tokens)))\n",
    "            textDigitsPerc.append(len(''.join(c for c in i if c in string.digits))/float(len(tokens)))\n",
    "\n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    \n",
    "    textLength_varname = str(col_name) + '_TextLength'\n",
    "    textWordsPerc_varname = str(col_name) + '_TextWordsPerc'\n",
    "    textPuncPerc_varname = str(col_name) + '_TextPuncPerc'\n",
    "    textDigitsPerc_varname = str(col_name) + '_TextDigitsPerc'\n",
    "    \n",
    "    listings[textLength_varname] = textLength\n",
    "    listings[textWordsPerc_varname] = textWordsPerc\n",
    "    listings[textPuncPerc_varname] = textPuncPerc\n",
    "    listings[textDigitsPerc_varname] = textDigitsPerc\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new2 = new.copy()\n",
    "for i in text_features:\n",
    "    new2 = create_txt_features(new[i], new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(pdseries, listings):\n",
    "    \n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    varname = str(col_name) + \"_LexicalDiversity\"\n",
    "    \n",
    "    lx_div = pd.Series([len(i)/len(set(i)) for i in pdseries])\n",
    "    listings[varname] = lx_div\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new3 = new2.copy()\n",
    "for i in text_features:\n",
    "    new3 = lexical_diversity(new2[i], new3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'house_rules-Topic0', u'house_rules-Topic1', u'house_rules-Topic2',\n",
       "       u'house_rules-Topic3', u'house_rules-Topic4', u'house_rules-Topic5',\n",
       "       u'house_rules-Topic6', u'house_rules-Topic7', u'house_rules-Topic8',\n",
       "       u'house_rules-Topic9',\n",
       "       ...\n",
       "       u'house_rules_TextPuncPerc', u'house_rules_TextDigitsPerc',\n",
       "       u'space_LexicalDiversity', u'description_LexicalDiversity',\n",
       "       u'neighborhood_overview_LexicalDiversity', u'notes_LexicalDiversity',\n",
       "       u'transit_LexicalDiversity', u'access_LexicalDiversity',\n",
       "       u'interaction_LexicalDiversity', u'house_rules_LexicalDiversity'],\n",
       "      dtype='object', length=305)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "filename = 'listings_augmented_' + str(now) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new3.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
