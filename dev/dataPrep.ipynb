{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation Notebook Consolidating All Data Prep code into linear process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from StringIO import StringIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings_original = pd.read_csv('Datasources/inside_airbnb/listings.csv')\n",
    "calendar_original = pd.read_csv('Datasources/inside_airbnb/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_columns(listings, cols):\n",
    "    chars = \"%$\"\n",
    "    for i in cols:\n",
    "        listings[i].astype(str).map(lambda x: x.rstrip(chars))\n",
    "        listings[i] = listings[i].apply(pd.to_numeric, errors='coerce')\n",
    "        listings[i].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "    return listings        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings = parse_columns(listings_original, ['host_response_rate', 'cleaning_fee',\\\n",
    "                                     'host_acceptance_rate','extra_people',\\\n",
    "                                     'weekly_price', 'monthly_price', 'security_deposit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to produce 4 listings dataframes (whole, holiday, wke, wkd) with listing mean price\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "def get_mean_price(cal, listings):\n",
    "    \n",
    "    cal['price'] = cal['price'].astype(str).map(lambda x: x.lstrip('$'))\n",
    "    cal['price'] = cal['price'].apply(pd.to_numeric, errors='coerce')\n",
    "    cal['price'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "    cal['date'] = pd.to_datetime(cal['date'])\n",
    "    cal['month'] = cal['date'].apply(lambda x: x.month)\n",
    "    cal['day'] = cal['date'].apply(lambda x: x.day)\n",
    "    cal['day_of_week'] = cal['date'].dt.weekday_name\n",
    "    \n",
    "    cl = calendar()\n",
    "    holidays = cl.holidays(start=cal['date'].min(), end=cal['date'].max())\n",
    "    \n",
    "    cal['holiday'] = cal['date'].isin(holidays)\n",
    "    cal = cal[(cal['date']>'2016-07-06')&(cal['date']<'2016-10-06')]\n",
    "    \n",
    "    c = cal.loc[cal.available!='f']\n",
    "    c = c[['listing_id','date','price','month','day_of_week','holiday']]\n",
    "    c=c.fillna(c.mean())\n",
    "    \n",
    "    c_hol = c[c['holiday']==True]\n",
    "    c_wke = c[(c['holiday']==False)&((c['day_of_week']=='Sunday')|(c['day_of_week']=='Saturday'))]\n",
    "    c_wkd = c[(~c.isin(c_hol['date']))&(~c.isin(c_wke['date']))]\n",
    "\n",
    "\n",
    "    price_hol_dict = {'price': c_hol.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c_hol.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c_hol.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c_hol.groupby(by='listing_id')['price'].fillna(0).std(),                 \n",
    "                  'skew_of_price': c_hol.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                     'median_price': c_hol.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "    price_wke_dict = {'price': c_wke.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c_wke.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c_wke.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c_wke.groupby(by='listing_id')['price'].fillna(0).std(),\n",
    "                  'skew_of_price': c_wke.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                     'median_price': c_wke.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "    price_wkd_dict = {'price': c_wkd.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c_wkd.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c_wkd.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c_wkd.groupby(by='listing_id')['price'].fillna(0).std(),\n",
    "                  'skew_of_price': c_wkd.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                     'median_price': c_wkd.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "    price_whole_dict = {'price': c.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c.groupby(by='listing_id')['price'].fillna(0).std(),\n",
    "                  'skew_of_price': c.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                       'median_price': c.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "\n",
    "    price_hol = pd.DataFrame(price_hol_dict)\n",
    "    price_wke = pd.DataFrame(price_wke_dict)\n",
    "    price_wkd = pd.DataFrame(price_wkd_dict)\n",
    "    price_c = pd.DataFrame(price_whole_dict)    \n",
    "    \n",
    "    price_hol = price_hol.reset_index()\n",
    "    price_wke = price_wke.reset_index()\n",
    "    price_wkd = price_wkd.reset_index()\n",
    "    price_c = price_c.reset_index()\n",
    "\n",
    "    listings_hol = listings.merge(price_hol, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_wke = listings.merge(price_wke, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_wkd = listings.merge(price_wkd, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_c = listings.merge(price_c, how='inner', left_on='id', right_on='listing_id')\n",
    "    \n",
    "    price_hol_new = price_hol.rename(columns = {'max_price': 'hol_max_price', 'min_price': 'hol_min_price', 'price': 'hol_price',\n",
    "                                           'skew_of_price': 'hol_skew_of_price', 'stdev_of_price': 'hol_stdev_of_price'})\n",
    "    price_wke_new = price_wke.rename(columns = {'max_price': 'wke_max_price', 'min_price': 'wke_min_price', 'price': 'wke_price',\n",
    "                                           'skew_of_price': 'wke_skew_of_price', 'stdev_of_price': 'wke_stdev_of_price'})\n",
    "    price_wkd_new = price_wkd.rename(columns = {'max_price': 'wkd_max_price', 'min_price': 'wkd_min_price', 'price': 'wkd_price',\n",
    "                                           'skew_of_price': 'wkd_skew_of_price', 'stdev_of_price': 'wkd_stdev_of_price'})\n",
    "    \n",
    "    listings_all = listings_c.merge(price_hol_new, how='inner', left_on='listing_id', right_on='listing_id')\n",
    "    listings_all = listings_all.merge(price_wke_new, how='inner', left_on='listing_id', right_on='listing_id')\n",
    "    listings_all = listings_all.merge(price_wkd_new, how='inner', left_on='listing_id', right_on='listing_id')\n",
    "    \n",
    "    #len(cal['listing_id'].astype(str).unique())\n",
    "    #count = len(c['listing_id'].astype(str).unique())\n",
    "    \n",
    "    #print('Due to the above filtering on calendar, the right total count of listings is: ' %(count))\n",
    "    \n",
    "    return listings_hol, listings_wke, listings_wkd, listings_c, listings_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings_hol, listings_wke, listings_wkd, listings = get_mean_price(calendar_original, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use n components in place of n topics when using gridsearchcv\n",
    "def create_topics(pdseries, listings):\n",
    "        corpus = pdseries.fillna('none')\n",
    "        \n",
    "        vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "        \n",
    "        data_vectorized = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        lda_model = LatentDirichletAllocation(n_topics=20,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "        \n",
    "        lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "        # column names\n",
    "        col_name = pd.DataFrame(pdseries).columns[0]\n",
    "        topicnames = [str(col_name) + \"-\" + \"Topic\" + str(i) for i in range(lda_model.n_topics)]\n",
    "\n",
    "        # index names\n",
    "        docnames = [str(col_name) + \"-\" + \"Doc\" + str(i) for i in range(len(corpus))]\n",
    "\n",
    "        # Make the pandas dataframe\n",
    "        df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "        # Get dominant topic for each document\n",
    "        dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "        df_document_topic[str(col_name) + \"-\" + 'Dominant_Topic'] = dominant_topic\n",
    "        \n",
    "        df_document_topic.index = [i for i in range(len(df_document_topic))]\n",
    "        \n",
    "        df_document_topic = df_document_topic.fillna(0)\n",
    "        \n",
    "        out = df_document_topic.merge(listings, left_index=True, right_index=True)\n",
    "        out = out.astype('str')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new = listings.copy()\n",
    "for i in text_features:\n",
    "    new = create_topics(listings[i], new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def create_txt_features(pdseries, listings):\n",
    "    \n",
    "    textLength = []\n",
    "    textWordsPerc = []\n",
    "    textPuncPerc = []\n",
    "    textDigitsPerc = []\n",
    "\n",
    "    for i in pdseries:\n",
    "        tokens = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        textLength.append(len(tokens))\n",
    "\n",
    "        if len(tokens)==0:\n",
    "            textWordsPerc.append(0)\n",
    "            textPuncPerc.append(0)\n",
    "            textDigitsPerc.append(0)\n",
    "\n",
    "        else:\n",
    "            textWordsPerc.append(len(i.split())/float(len(tokens)))\n",
    "            textPuncPerc.append(len(''.join(c for c in i if c in string.punctuation))/float(len(tokens)))\n",
    "            textDigitsPerc.append(len(''.join(c for c in i if c in string.digits))/float(len(tokens)))\n",
    "\n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    \n",
    "    textLength_varname = str(col_name) + '_TextLength'\n",
    "    textWordsPerc_varname = str(col_name) + '_TextWordsPerc'\n",
    "    textPuncPerc_varname = str(col_name) + '_TextPuncPerc'\n",
    "    textDigitsPerc_varname = str(col_name) + '_TextDigitsPerc'\n",
    "    \n",
    "    listings[textLength_varname] = textLength\n",
    "    listings[textWordsPerc_varname] = textWordsPerc\n",
    "    listings[textPuncPerc_varname] = textPuncPerc\n",
    "    listings[textDigitsPerc_varname] = textDigitsPerc\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new2 = new.copy()\n",
    "for i in text_features:\n",
    "    new2 = create_txt_features(new[i], new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(pdseries, listings):\n",
    "    \n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    varname = str(col_name) + \"_LexicalDiversity\"\n",
    "    \n",
    "    lx_div = pd.Series([len(i)/len(set(i)) for i in pdseries])\n",
    "    listings[varname] = lx_div\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new3 = new2.copy()\n",
    "for i in text_features:\n",
    "    new3 = lexical_diversity(new2[i], new3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'house_rules-Topic0', u'house_rules-Topic1', u'house_rules-Topic2',\n",
       "       u'house_rules-Topic3', u'house_rules-Topic4', u'house_rules-Topic5',\n",
       "       u'house_rules-Topic6', u'house_rules-Topic7', u'house_rules-Topic8',\n",
       "       u'house_rules-Topic9',\n",
       "       ...\n",
       "       u'house_rules_TextPuncPerc', u'house_rules_TextDigitsPerc',\n",
       "       u'space_LexicalDiversity', u'description_LexicalDiversity',\n",
       "       u'neighborhood_overview_LexicalDiversity', u'notes_LexicalDiversity',\n",
       "       u'transit_LexicalDiversity', u'access_LexicalDiversity',\n",
       "       u'interaction_LexicalDiversity', u'house_rules_LexicalDiversity'],\n",
       "      dtype='object', length=309)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_grammar(pdseries, listings):\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.tag import pos_tag, map_tag\n",
    "    from collections import Counter\n",
    "      \n",
    "    df = pd.DataFrame()\n",
    "    for text in pdseries:\n",
    "        \n",
    "        col_name = pd.DataFrame(pdseries).columns[0]\n",
    "        \n",
    "        \n",
    "        tokenized_text = nltk.word_tokenize(text.decode('utf-8'))\n",
    "        grammar = [i[1] for i in nltk.pos_tag(tokenized_text, tagset='universal')]\n",
    "        \n",
    "        counter = Counter(grammar)\n",
    "        fr = pd.DataFrame(counter, index=[0])\n",
    "        fr.columns = [str(col_name) + '_' + str(i) for i in fr.columns]\n",
    "        \n",
    "        fr2 = fr/len(tokenized_text)\n",
    "        fr2.columns = [str(i) + '_tokens_sum_ratio' for i in fr2.columns]\n",
    "        \n",
    "        fr3 = pd.concat([fr, fr2], ignore_index=True)\n",
    "        \n",
    "        df = pd.concat([df, fr3], ignore_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    df = df.fillna(0)\n",
    "        \n",
    "    return listings.merge(df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new4 = new3.copy()\n",
    "for i in text_features:\n",
    "    new4 = extract_grammar(new3[i], new4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_Clusterer(pdseries, listings):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(pdseries)\n",
    "    true_k = 10\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "    model.fit(X)\n",
    "    \n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    varname = str(col_name) + \"_KmeansCluster\"\n",
    "    \n",
    "    listings[varname] = pd.Series(model.labels_)\n",
    "    listings[varname] = listings[varname].fillna(0)\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\n",
    "new5 = new4.copy()\n",
    "for i in text_features:\n",
    "    new5 = kmeans_Clusterer(new4[i], new5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_to_set(x):\n",
    "    c = set()\n",
    "    for w in x[1:-1].split(\",\"):\n",
    "        c.add(w)\n",
    "        \n",
    "    return c\n",
    "\n",
    "def has_amenity(x, amen_):\n",
    "    if amen_ in x:\n",
    "        return 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_amenities(listings):\n",
    "    listings['amenities_set'] = listings['amenities'].fillna('{}').map(string_to_set)\n",
    "    all_amenities = set()\n",
    "    \n",
    "    for idx in listings['amenities'].fillna('{}').map(string_to_set).index:\n",
    "        all_amenities = all_amenities.union(listings['amenities'].fillna('{}').map(string_to_set)[idx])\n",
    "        \n",
    "    for amen in all_amenities:\n",
    "        listings['has' + amen] = 0\n",
    "        listings['has' + amen] = listings['amenities_set'].map(lambda x: has_amenity(x, amen))\n",
    "\n",
    "    has_amenties_list = []\n",
    "    for amen in all_amenities:\n",
    "        has_amenties_list.append('has' + amen)\n",
    "\n",
    "    listings[has_amenties_list] = listings[has_amenties_list].fillna(0)\n",
    "    \n",
    "    return listings      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new6 = new5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new6 = add_amenities(new6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_host_verifications(listings):\n",
    "    a = listings['host_verifications'].map(lambda x: x[1:-1]).map(lambda j: j.split(',')).map(lambda k: set(k))\n",
    "    all_host_verifications = set()\n",
    "    \n",
    "    for w in a.index:\n",
    "        all_host_verifications = all_host_verifications.union(a[w])\n",
    "        \n",
    "    for w in all_host_verifications:\n",
    "        listings['uses' + w] = 0\n",
    "        listings['uses' + w] = a.map(lambda x: has_amenity(x, w))\n",
    "    \n",
    "    \n",
    "    uses_verification_list = []\n",
    "    \n",
    "    for veri in all_host_verifications:\n",
    "        uses_verification_list.append('uses' + veri)\n",
    "    \n",
    "    listings[uses_verification_list] = listings[uses_verification_list].fillna(0)\n",
    "    \n",
    "    return listings      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new7 = new6.copy()\n",
    "new7 = add_amenities(new7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_distance_from_ocean(listings):\n",
    "    listings['distance_from_ocean'] = 0\n",
    "    \n",
    "    for w in listings.index:\n",
    "        p = float(listings['latitude'][w])\n",
    "        q = float(listings['longitude'][w])\n",
    "        lon_diff = (q + 117.235585)*np.pi/180\n",
    "        lat_diff = (p - 32.802458)*np.pi/180\n",
    "        a = np.sin(lat_diff/2)**2 + np.cos(p*np.pi/180)*np.cos(32.802458*np.pi/180)*(np.sin(lon_diff/2)**2)\n",
    "        c = np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        d = 6371*float(c)\n",
    "        listings['distance_from_ocean'][w] = d\n",
    "        \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new8 = new7.copy()\n",
    "new8 = add_distance_from_ocean(new8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "today = datetime.date.today()\n",
    "count+=1\n",
    "filename = 'listings_augmented_' + str(today) + '_V' + str(count) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"For the date, you can use datetime.date.today() or datetime.datetime.now().date().\n",
    "\n",
    "For the time, you can use datetime.datetime.now().time().\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new8.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Listings Shape at Each Iteration\"\n",
    "print listings_original.shape\n",
    "print listings.shape\n",
    "print new.shape\n",
    "print new2.shape\n",
    "print new3.shape\n",
    "print new4.shape\n",
    "print new5.shape\n",
    "print new6.shape\n",
    "print new7.shape\n",
    "print new8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
