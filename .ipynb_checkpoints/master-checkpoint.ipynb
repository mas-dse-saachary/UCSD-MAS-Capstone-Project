{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master File Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/17 -- I created this notebook to apply all the basic data manipulations on the listings and calendar notebook in a separate place so a standard version of the 'Master' dataframe can be loaded from other notebooks rather than repeating the data transformation steps every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook's purpose is to output the 'master.csv' file in the main directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neighbourhoods.geo.json',\n",
       " 'reviews.csv',\n",
       " '.DS_Store',\n",
       " 'neighbourhoods.geojson',\n",
       " 'listings_summ.csv',\n",
       " 'neighbourhoods.csv',\n",
       " 'stations.geojson',\n",
       " 'listings.csv',\n",
       " 'calendar.csv',\n",
       " 'reviews_summ.csv',\n",
       " 'neighbourhoods.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"Datasources/inside_airbnb\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings = pd.read_csv('inside_airbnb/listings.csv')\n",
    "#list_summ = pd.read_csv('inside_airbnb/listings_summ.csv')\n",
    "#neighborhoods = pd.read_csv('inside_airbnb/neighbourhoods.csv')\n",
    "#reviews = pd.read_csv('inside_airbnb/reviews.csv')\n",
    "#reviews_summ = pd.read_csv('inside_airbnb/reviews_summ.csv')\n",
    "calendar = pd.read_csv('inside_airbnb/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings = listings.loc[listings.room_type == 'Entire home/apt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cal = calendar.copy()\n",
    "cal['price'] = cal['price'].astype(str).map(lambda x: x.lstrip('$'))\n",
    "#Transform Price from Object to Numeric Data Type\n",
    "cal['price'] = cal['price'].apply(pd.to_numeric, errors='coerce')\n",
    "cal['price'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Just using the calendar listingid, date and price to merge with listings\n",
    "c = cal.loc[cal.available!='f']\n",
    "c = c[['listing_id','date','price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master = listings.merge(c, how='inner', left_on='id', right_on='listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(master)\n",
    "pq.write_table(table, 'master.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Compression 'snappy' not available.  Options: ['GZIP', 'UNCOMPRESSED']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-207dc02dc57f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Trying Parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'master.parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fastparquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(self, fname, engine, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m         to_parquet(self, fname, engine,\n\u001b[0;32m-> 1649\u001b[0;31m                    compression=compression, **kwargs)\n\u001b[0m\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m     @Substitution(header='Write out the column names. If a list of strings '\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/pandas/io/parquet.pyc\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet must have string column names\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/pandas/io/parquet.pyc\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, df, path, compression, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             self.api.write(path, df,\n\u001b[0;32m--> 116\u001b[0;31m                            compression=compression, **kwargs)\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/fastparquet/writer.pyc\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(filename, data, row_group_offsets, compression, file_scheme, open_with, mkdirs, has_nulls, write_index, partition_on, fixed_text, append, object_encoding, times)\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_scheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'simple'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         write_simple(filename, data, fmd, row_group_offsets,\n\u001b[0;32m--> 846\u001b[0;31m                      compression, open_with, has_nulls, append)\n\u001b[0m\u001b[1;32m    847\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfile_scheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'hive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'drill'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/fastparquet/writer.pyc\u001b[0m in \u001b[0;36mwrite_simple\u001b[0;34m(fn, data, fmd, row_group_offsets, compression, open_with, has_nulls, append)\u001b[0m\n\u001b[1;32m    715\u001b[0m                    else None)\n\u001b[1;32m    716\u001b[0m             rg = make_row_group(f, data[start:end], fmd.schema,\n\u001b[0;32m--> 717\u001b[0;31m                                 compression=compression)\n\u001b[0m\u001b[1;32m    718\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0mfmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_groups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/fastparquet/writer.pyc\u001b[0m in \u001b[0;36mmake_row_group\u001b[0;34m(f, data, schema, compression)\u001b[0m\n\u001b[1;32m    612\u001b[0m                 \u001b[0mcomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             chunk = write_column(f, data[column.name], column,\n\u001b[0;32m--> 614\u001b[0;31m                                  compression=comp)\n\u001b[0m\u001b[1;32m    615\u001b[0m             \u001b[0mrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     rg.total_byte_size = sum([c.meta_data.total_uncompressed_size for c in\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/fastparquet/writer.pyc\u001b[0m in \u001b[0;36mwrite_column\u001b[0;34m(f, data, selement, compression)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0mbdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompress_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/fastparquet/compression.pyc\u001b[0m in \u001b[0;36mcompress_data\u001b[0;34m(data, compression)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompressions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         raise RuntimeError(\"Compression '%s' not available.  Options: %s\" %\n\u001b[0;32m---> 87\u001b[0;31m                 (algorithm, sorted(compressions)))\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompressions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Compression 'snappy' not available.  Options: ['GZIP', 'UNCOMPRESSED']"
     ]
    }
   ],
   "source": [
    "#Trying Parquet\n",
    "master.to_parquet('master.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Modifications to master to build basic features on transit column requested by Sankarshan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in master['transit']:\n",
    "    lst.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "master['transit_length'] = (pd.Series([len(i) for i in lst]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(text)/len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "master['transit_variety'] = pd.Series([lexical_diversity(i) for i in lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master['transit_vocab_size'] = pd.Series([len(set(i)) for i in lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content)/len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:3: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-75a1bb48defa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#the percentage of non stopword words in every transit comment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmaster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transit_content_fraction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontent_fraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-59dd02334cc8>\u001b[0m in \u001b[0;36mcontent_fraction\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcontent_fraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#the percentage of non stopword words in every transit comment\n",
    "master['transit_content_fraction'] = [content_fraction(i) for i in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "reviewTextLength = []\n",
    "reviewTextWordsPerc = []\n",
    "reviewTextPuncPerc = []\n",
    "reviewTextDigitsPerc = []\n",
    "\n",
    "for i in lst:\n",
    "    tokens = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "    reviewTextLength.append(len(tokens))\n",
    "    \n",
    "    if len(tokens)==0:\n",
    "        reviewTextWordsPerc.append(0)\n",
    "        reviewTextPuncPerc.append(0)\n",
    "        reviewTextDigitsPerc.append(0)\n",
    "\n",
    "    else:\n",
    "        reviewTextWordsPerc.append(len(i.split())/float(len(tokens)))\n",
    "        reviewTextPuncPerc.append(len(''.join(c for c in i if c in string.punctuation))/float(len(tokens)))\n",
    "        reviewTextDigitsPerc.append(len(''.join(c for c in i if c in string.digits))/float(len(tokens)))\n",
    "\n",
    "master['transitTextLength'] = reviewTextLength\n",
    "master['transitTextWordsPerc'] = reviewTextWordsPerc\n",
    "master['transitTextPuncPerc'] = reviewTextPuncPerc\n",
    "master['transitTextDigitsPerc'] = reviewTextDigitsPerc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master.to_parquet('master_transit.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
